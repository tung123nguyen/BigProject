{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40686f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    PyMuPDFLoader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d3af1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pypdf_loader = PyPDFLoader(\"../../data/pdf/2510.20809v1.pdf\")\n",
    "    pypdf_docs = pypdf_loader.load()\n",
    "    print(f\"{len(pypdf_docs)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98cb3188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Real Deep Research for AI, Robotics and Beyond\\nXueyan Zou∗1, Jianglong Ye∗1, Hao Zhang2, Xiaoyu Xiang3, Mingyu Ding5, Zhaojing Yang1,\\nYong Jae Lee4, Zhuowen Tu1, Sifei Liu2, Xiaolong Wang1\\n1UC San Diego , 2NVIDIA , 3META , 4UW-Madison , 5UNC ,\\nhttps://realdeepresearch.github.io/\\nAbstract\\nWith the rapid growth of research in modern AI and robotics—now producing over 10,000 papers\\nannually—it has become increasingly difficult for researchers to stay up to date. Fast-evolving trends, the\\nrise of interdisciplinary work, and the need to explore domains beyond one’s expertise all contribute to\\nthis challenge. To address these issues, we propose a generalizable pipeline capable of systematically\\nanalyzing any research area: identifying emerging trends, uncovering cross-domain opportunities, and\\noffering concrete starting points for new inquiry. In this work, we present Real Deep Research (RDR)—a\\ncomprehensive framework applied to the domains of AI and robotics, with a particular focus on foundation\\nmodels and robotics advancements. We also briefly extend our analysis to other areas of science. The\\nmain paper details the construction of the RDR pipeline, while the appendix provides extensive results\\nacross each analyzed topic. We hope this work could shed lights on researchers who works in the filed of\\nAI and beyond.\\nFigure 1 | Real Deep Research enables: (1) generating surveys for specific research focuses or\\nperspectives; (2) analyzing topic trends over time; (3) mapping interdisciplinary research landscapes; and\\n(4) retrieving high-impact papers relevant to a given topic. (Each dot represents a paper, and each sphere\\ndenotes a topic cluster. The cluster keywords and trend information are automatically generated by RDR)\\n* Indicate core contribution.\\narXiv:2510.20809v1  [cs.AI]  23 Oct 2025'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='1. Introduction\\nThe fields of AI and robotics have experienced exponential growth in recent years, while researchers\\ncontinue to face the constraint of limited time and attention. This work is motivated by the authors’\\nneed to efficiently survey research areas, stay up to date with rapidly evolving trends, identify promising\\ninterdisciplinary opportunities, and familiarize themselves with the latest developments on a given topic.\\nIn response to this need, we develop a systematic analysis tool designed to help users quickly navigate\\nand adapt to any research area or topic. We begin by applying our approach to the fields of AI and robotics,\\nconducting an in-depth analysis with a focus on foundation models and robotics research. To broaden our\\nexploration and uncover emerging areas of interest, we also extend our analysis to natural sciences and\\nformal sciences, offering a glimpse into recent developments beyond our core domains.\\nAlthough our intentions are well-founded, it is important to acknowledge existing efforts in this space.\\nOn the one hand, there are high-quality survey papers written by domain experts [12, 83]; on the other\\nhand, a few recent works have explored automated research pipelines [3, 115]. Expert-written surveys\\noffer depth and accuracy, but require significant manual effort and cannot easily adapt to the fast-paced\\nevolution of research. Meanwhile, current automated approaches often lack domain-specific knowledge\\nand expert insight, limiting their usefulness and relevance to researchers. Our work aims to bridge this\\ngap by combining systematic automation with meaningful, expert-informed analysis.\\nTherefore, in addition to building an effective pipeline for Real Deep Research, our goal is to make\\nthe tool robust and insightful enough to support top-tier researchers in tracking emerging trends and\\nengaging with unfamiliar research areas. A key focus of our work is interdisciplinary exploration—helping\\nresearchers identify underexplored intersections between fields that present promising opportunities for\\ncross-domain collaboration.\\nAs shown in Fig. 1, the visualization displays individual papers, clustered research topics, and their\\ncorresponding trends. At a glance, it becomes clear that areas such as teleoperation, dexterous manipula-\\ntion, and open-source robotics are emerging as promising directions, whereas traditional reinforcement\\nlearning appears to be declining in momentum. As researchers in the robotics field, we find that these trend\\ninsights align well with our domain knowledge and provide valuable guidance for identifying impactful\\nresearch opportunities. We summarize the key contributions of this paper as follows:\\n1. We propose the Real Deep Research (RDR) pipeline, a systematic framework for exploring and\\nanalyzing any research area in depth.\\n2. Leveraging domain expertise, we deliver high-quality survey outputs in the fields of AI and robotics,\\nproviding valuable insights for researchers and practitioners.\\n3. We quantitatively evaluate the RDR pipeline and demonstrate its advantages over existing commercial\\nlarge language model tools within the targeted research domains.\\n2. Related Work\\nSurveys of Foundation Models. In recent years, a number of survey studies have systematically reviewed\\nfoundation models across different domains [12, 55, 83, 141, 234, 298], including natural language\\nprocessing [20, 288], computer vision [141, 269], graph learning [234], and robotics [153, 242, 246, 267].\\nHowever, these surveys require extensive manual effort and become outdated quickly due to the rapid\\nprogress of foundation models. Unlike such static surveys, our goal is to design a framework that can\\nautomatically analyze thousands of papers and provide an always up to date understanding of different\\nresearch areas.\\nLLMs in Scientific Research. Large language models (LLMs) have been applied across various stages\\nof scientific research [145, 161, 190, 217], including idea generation [6, 222], coding [155, 245], paper\\n2'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='reviewing [124, 145], and predicting experimental results [149, 156]. Among these stages, literature\\nanalysis plays a central role, involving tasks such as paper retrieval, clustering, and topic trend analysis.\\nHowever, traditional literature search tools such as Google Scholar rely mainly on lexical matching and\\nstruggle with tasks that require deeper semantic reasoning. This has motivated researchers to leverage\\nLLMs for literature analysis [3, 76, 115, 196]. For example, SciLitLLM [115] employs supervised\\nlearning to build a specialized LLM for scientific literature understanding; PaSa [76] uses reinforcement\\nlearning with synthetic data to train an LLM agent that can answer complex scholarly queries. Unlike\\nprior work that focuses mainly on research question answering, our approach targets a broader and\\nsystematic understanding of entire research areas. We highlight not only semantic reasoning over large\\ncollections of papers but also automatic analysis of research trends, offering researchers a transparent and\\nevidence-based view of the literature.\\nKnowledge Organization and Discovery. It has been shown that LLMs are capable of clustering\\ndocuments [219, 281] and uncovering latent topics [114, 177]. For example, Knowledge Navigator [97]\\ncombines LLMs with clustering techniques to organize and structure documents for scientific literature\\nsearch; SciTopic [114] enhances LLMs in identifying topic structures by refining document embeddings.\\nBeyond knowledge organization, recent research [62, 105, 106] also studies the trend of high-impact\\nresearch topics. Our work introduces a novel approach by leveraging the reasoning capabilities of LLMs\\nand the embedding representations of foundation models, which leads to more accurate and semantic\\nknowledge organization. Built on this knowledge structure, our framework enables analysis of past and\\nfuture research trends and supports inspection of connections between topics, providing valuable insights\\ninto scientific directions.\\n3. Method\\nIn the Methods section, we focus specifically on the domains of foundation models and robotics to\\nprovide a comprehensive overview of how we conduct Real Deep Research using expert knowledge. As\\nillustrated in Fig. 2, the embedding-based analysis pipeline consists of four main components:(1) Data\\nPreparation (Sec. 3.1), (2) Content Reasoning (Sec. 3.2), (3) Content Projection (also in Sec. 3.2), and (4)\\nEmbedding Analysis (Sec. 3.4). This pipeline is powered by a suite of large language and multimodal\\nmodels (LLMs/LMMs) for content extraction and reasoning, and is designed to be generalizable for\\nthe automated analysis of other research domains in the future. The following sections introduce each\\ncomponent in detail.\\n3.1. Data Preparation.\\nSelection. To systematically investigate the integration of foundation models and robotics at scale, we\\nfocus on emerging trends and research priorities in both academia and industry. To capture the latest\\ndevelopments, we review recent publications from leading conferences in computer vision, robotics,\\nand machine learning. Specifically, we collect papers via web crawling from top conference venues\\n(CVPR, ECCV, ICCV, CoRL, RSS, ICRA, NeurIPS, etc.) as well as from industry research platforms\\n(Nvidia, Meta, and OpenAI, etc.). This curated corpus comprehensively overviews the research contents\\nin foundation models and robotics, highlighting key technical advancements, existing challenges, and\\nfuture research directions. Specifically, we collect paper titles, authors, abstracts, and PDF links directly\\nfrom conference and company websites. Then, we apply an area filtering process on paper titles and\\nabstracts using an efficient LLM with a predefined set of criteria to ensure relevance to this study.\\nArea Filtering. We define the collected paper set as P, while it generally fall under the broad area of\\nvision, language, machine learning, and robotics, it is not guaranteed that each paper directly aligns with\\nthe specific focus of our work, such as foundation models (D 𝑓) and robotics (D𝑟). To address this, we\\n3'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='Figure 2 | Pipeline of the proposed method on filtering and projecting thousands of papers to the\\nembedding space for future analysis.\\nintroduce Area Filtering—a step that leverages an efficient LLM with curated prompts—to identify papers\\nrelevant to our research scope. To ensure a correct filtering, we first define the scope of foundation models\\nand robotics, clarifying technical boundaries between domains. Below are the prompts that we designed\\nfor our research focus:\\nFoundation Model Definition:\\n”Research involving deep learning models (especially\\ntransformer-based) trained on large amounts of data and capable of fitting\\ngeneralized factual realities.\\nThese models typically serve as versatile backbones\\nfor a variety of downstream tasks across multiple domains.”\\nKey Indicators:\\n- Large Multimodal Models (LMM)\\n- Large Language Models (LLM) ...\\nRobotics Definition:\\n”Research involving hardware systems equipped with input\\nsensors and mechanical kinematics capable of producing joint movements.\\nThese\\nsystems are controlled by learning-based algorithms that facilitate automatic or\\nrobust mappings from sensory inputs to actuator outputs.”\\nKey Indicators:\\n- Reinforcement Learning in robotic contexts\\n- Imitation Learning for physical systems ...\\nAfter filtering using an efficient LLM, the resulting set of papers (P′) belongs to either the foundation\\nmodel domain (D 𝑓), the robotics domain (D𝑟), or both. Formally we write P′ = {𝑝| 𝑝∈D 𝑓∪D𝑟}.\\n3.2. Content Reasoning.\\nGiven the filtered papers P′ in the domains of foundation models and robotics, an in-depth analysis\\nis required to narrow the position of each paper. Guided by domain experts in foundation models\\nand robotics, we define perspectives that align with established domain structures, emerging trends,\\nand evolving knowledge. Beyond predefined perspectives, our pipeline supports future user-defined\\nperspectives, allowing adaptation to new research questions. In the following paragraphs, we will depict\\nthe general structure, trends, and knowledge of the foundation model and robotics in preparation for\\nanalyzing the research works under P′.\\nFoundation Model. A foundation model’s development are systematically analyzed through five fun-\\ndamental perspectives in this work: Input (I), Modeling (M), Output (O), Objective (W), and Learning\\nRecipe (R). We have shown some main perspectives examples in Fig. 3. This structured representation fa-\\ncilitates a comprehensive analysis of the foundation model. Below is the formal writing for the procedure:\\nD𝑃′\\n𝑓=\\nØ\\n𝑝∈P′\\n𝐹(𝑝),\\n𝐹(𝑝) = LLM(𝑝| I, M, O, W, R),\\nwhere LLM represents the large multimodal model, and D𝑃′\\n𝑓denotes the perspective projection of the\\ngiven papers in P′, focused on foundation model research. In the following paragraphs, we provide a\\n4'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='Figure 3 | Perspective analysis of foundation model research, which primarily includes (1) Input, (2)\\nModeling, (3) Output, etc., shown in the figure.\\nformal definition of each perspective:\\nInput (I). The input processing for a foundation model generally involves raw data and a tokenization\\nprocedure. Standard input sources include images, videos, audio, LiDAR, etc., with tokenization performed\\nthrough transformations and neural networks.\\nModeling (M). With input settled for a foundation model, the modeling part is responsible for extracting\\ncritical knowledge from the input, reasoning, and decoding to the output space. It is the critical procedure\\nto transfer input knowledge to output.\\nOutput (O). The task determines the decoding space according to the input and modeling, this is the final\\nstep to decode the latent representation to the output used for loss computation or the final interaction.\\nObjective (W). To fit a foundation model with the corresponding input, and output, the given model\\narchitecture is constrained by the learning objective, this fits the model distribution in alignment with the\\ntransformation given the task(s).\\nRecipe (R). The recipe is used as the cookbook on how to tune the model weight with input, output, and\\nobjective. It controls the training stage, convergence speed, and updated parameters.\\nRobotics. For research work in robotics, the core perspective shifts to emphasize hardware and interaction\\nwithin real-world environments. We define five key perspectives to map each paper within the broader\\nlandscape of robotic applications: Input Sensor (S), Physical Body (B), Joint Output (J), Action Space\\n(A), and Environment (E). An example of core perspectives is illustrated in Fig. 4. These perspectives\\ncollectively define how robots perceive, act, and interact within the physical world. The procedure could\\nbe formally written as:\\nD𝑃′\\n𝑟=\\nØ\\n𝑝∈P′\\n𝐹(𝑝),\\n𝐹(𝑝) = LMM(𝑝| S, B, J, A, E),\\nwhere D𝑃′\\n𝑟represents the perspective projection of the given papers in P′, providing a structured framework\\nfor analyzing robotics research. We show the concrete definition in the following:\\nInput Sensor (S). Input sensors are hardware devices that measure physical quantities or environmental\\nconditions and convert them into digital signals that can be processed by the robot’s control system. They\\nserve as the robot’s interface with its environment.\\nPhysical Body (P). A physical body in robotics refers to the mechanical structure and architecture that\\nenables physical interaction with the environment. This physical manifestation determines how motor\\ncommands translate into real-world forces, movements, and environmental manipulations.\\nAction Space (A). The action space is the set of all permissible actions a robot can select in a given\\ncontext, ranging from low-level joint commands to high-level behaviors (e.g., “walk” or “grasp”). Each\\n5'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='Figure 4 | Perspective analysis of robotics research, which primarily includes (1) Input, (2) Modeling, (3)\\nOutput, etc., shown in the figure.\\nchosen action is ultimately executed as a joint output, bridging decision-making to physical movement.\\nJoint Output (J). Joint output refers to the physical movement or configuration of a robot’s joints resulting\\nfrom executed motor commands. It translates control signals (e.g., torque or velocity) into mechanical\\nmotion, allowing the robot to directly interact with and manipulate its environment.\\nEnvironment (E). The environment encompasses the physical space where a robot operates, characterized\\nby its spatial layout, structural features, and contextual elements (e.g., furniture, tools, obstacles) that\\nshape the task-specific challenges and opportunities the robot encounters.\\nGiven the predefined perspective definition, we use the following prompt to extract each perspective\\nfrom the paper:\\nCan you analyze the paper contents according to the following perspectives:\\n(1)\\nDefinition 1, (2) Definition 2, (3) Definition 3, ...\\nAfter analysis, please identify each of the perspectives in the paper, and return\\nthe answer in the following format:\\n{\"perspective 1\":\\nplain text, \"perspective 2\":\\nplain text, \"perspective 3\":\\nplain text, ...}\\n3.3. Content Projection.\\nGiven the extracted contents from research papers guided by our defined perspective, we aim to project\\nnatural language descriptions into an informative latent space. This projection enables large-scale analysis\\nof current research in foundation models and robotics while revealing potential future research directions.\\nMotivated by recent advancements in large language model-based embedding models, we employ a pre-\\ntrained embedding foundation model G to project D𝑃′\\n𝑟(processed robotics papers’ content) and D𝑃′\\n𝑓\\n(processed foundation model papers’ content) from natural language space into a more abstract embedding\\nspace. The embedding model maps text into a high-dimensional vector space where semantically similar\\nconcepts occupy proximate regions.\\nWe formally define this projection procedure as follows: For any text snippet 𝑥∈D, its embedding\\nis computed as: 𝑣𝑥= G(𝑥) ∈R𝑑. Our core assumption is that by projecting paper contents through\\nthis perspective-aware embedding process and analyzing them in the high-dimensional manifold, we\\ncan uncover meaningful patterns, research trends, and potential gaps in the literature through systematic\\nvisualization and clustering analysis.\\n3.4. Embedding Analysis.\\nThe goal of embedding analysis is to structure the understanding of previously extracted embeddings. The\\npipeline for embedding analysis contains three components: (1) Clustering on the extracted embeddings\\n6'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='and analyzing the main concept from each cluster. (2) Structured the concept for each cluster to formulate\\nan informative table. (3) Given the structured understanding, we trace back to the reference papers.\\nClustering for Embeddings. We first embed every paper to obtain its vector representation 𝑉and\\npartition the corpus into 𝑘clusters. From each cluster, we then draw a random sample of 50 papers and\\nfeed their text to a reasoning-based model with the prompt:\\nCan you summarize the following contents into three distinct keywords:\\nHere is one\\nexample output:\"keyphrase1, keyphrase2, keyphrase3\".\\nThe output should be short and\\nprecise, with a single output for all papers.\\nThe model returns three compact key phrases that capture the cluster’s core theme, giving every paper\\nboth a cluster label and an interpretable set of keywords for subsequent analysis.\\nStructuring for Thoughts. With clustered embeddings and their associated topic keywords in place,\\nthe next step is to generate a structured survey for the given research area. To accomplish this, we leverage\\nthe o3 language model, using the clustered keywords as prompts to guide the formulation of the final\\nsurvey content. Incorporating the clustering results into the prompt ensures that the generated text remains\\ngrounded in the actual structure of the research landscape, enhancing both coherence and relevance. We\\nuse the following prompt to produce the final output:\\nThose are summarized keywords for a number of science papers clustered by abstract\\ncontents, however they are ambiguous, contents may overlap between clusters, can you\\nsummarize the information in a more structured way for audience with the following\\ncriteria:\\n...\\n4. Analysis\\nIn this section, we conduct a comprehensive qualitative analysis of the conclusions drawn in this work\\nfrom the following perspectives: (1) Embedding analysis for general research areas. (2) Embedding\\nanalysis within specific perspective. (3) Trend analysis of research focus over time. (4) Knowledge graph\\nexploration across different research areas. (5) Retrieval examples based on embeddings. This pipeline\\nwill enable a researcher to dive into any research area, identify what to explore, and determine the specific\\npapers to focus on.\\nEmbedding Analysis - General. The output of the embedding analysis is a comprehensive survey tailored\\nto the featured research domain. This survey is organized into major categories and sub-categories, each\\ndetailing the specific topics covered. Rather than generating the survey content via LLM, we leverage the\\nclustering results from the embedding analysis to guide its structure and scope. Additionally, for each\\nsub-topic, we include the most relevant citations to provide readers with direct references for further\\nexploration. We have provide the full survey for Foundation Model, Robotics, Computer Vision, Natural\\nlanguage processing, and machine learning in Appendix.\\nCat.\\nSub-category\\nWhat is covered\\nTypical examples\\nCluster\\n1. Perception & Mapping [71, 168, 185, 277, 294]\\n1.1 Multimodal sensor fusion\\n[14, 123, 224, 256, 303]\\nFuse heterogeneous sensors for\\nricher scene understanding\\nLiDAR–Camera Fusion; Radar–Camera Fusion;\\nV2X Cooperative Perception ...\\n0,6,7,8,14,16\\n1.2 3D reconstruct/occupancy\\n[85, 133, 140, 175, 294]\\nBuild dense or sparse geometric\\nmaps for localisation\\n3-D SLAM & Reconstruction; 3-D Occupancy;\\nEfficient 3-D Representation\\n0,8,16\\n1.3 BEV / top-view mapping\\n[19, 93, 120, 256, 286]\\nBird’s-eye or top-down\\nrepresentations for planning\\nBEV Perception; V2X Collaborative Perception\\n0,14,16\\n... ...\\nEmbedding Analysis - Perspective. After establishing a clear overview of the domain, we analyze\\n7'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='it through a targeted perspective to expose structure and problem formulations. As introduced in the\\nMethods section, our perspective analysis uses embedding-based clustering to organize works along a\\nchosen axis. In this study we focus on foundation models and robotics, examining how each community\\nformulates its problems. Below we illustrate the robotics case from the viewpoint of action space. This\\nperspective-guided embedding analysis yields a deeper understanding of the domain and a high-level map\\nof how researchers approach and solve its problems. We also provide the full perspective for foundation\\nmodel and robotics in Appendix.\\nCategory\\nSub-category\\nWhat is covered\\nTypical examples\\nCluster\\n1. Continuous Low-Level Actuation\\n1.1 Joint-space\\ncommands\\nDirect numerical inputs to\\nindividual joints or actuators,\\nbounded by hardware limits.\\njoint torques/positions/velocities;\\nhigh-dimensional joint commands;\\nbounded control inputs; finger-joint\\nconfigs; parametrised joint trajectories\\n0, 4, 6, 10, 12,\\n14, 18\\n1.2 Vehicle / body\\ndynamics commands\\nLow-level controls that change a\\nmobile base, ground-vehicle or\\naerial body state.\\nsteering angle; throttle / acceleration;\\nbraking; linear & angular velocity;\\nbody-rate thrust; speed/direction for\\nlocomotion; lane-keeping\\n0, 1, 7, 10, 12,\\n13, 15\\n... ...\\nTrend Analysis. Once we understand each domain and its key sub-perspectives, the next step is to\\nassess topic momentum. Our trend analysis highlights which areas are accelerating and which have been\\nthoroughly explored in recent years, giving a practical starting point when entering a new field. In robotics\\n(see figure), the trajectories suggest that teleoperation, dexterous manipulation, and low-cost open-source\\nrobotics are currently rising, while traditional reinforcement learning and skill-based manipulation appear\\ncomparatively mature or show slowing activity. This will guide the researchers to smoothly enter a new\\nfield. We provide the full trend analysis in Appendix. A for Computer Vision, NLP, Robotics, and Machine\\nLearning.\\n1. Legged Locomotion, Reinforcement Learning,\\nSim-to-Real Transfer\\n2. Teleoperation, Dexterous Manipulation, Low-Cost\\nOpen-Source Robotics\\n3. Language-Conditioned Manipulation,\\nVision-Language-Action Models, 3D Scene Grounding\\n... ...\\n... ...\\n... ...\\n28. Offline Reinforcement Learning, Robotic Skill\\nLearning, Continual Adaptation\\n29. Trajectory Prediction, Safety-Critical Scenario\\nGeneration, Autonomous Driving Simulation\\n30. Robotic Reinforcement Learning, Skill-Based\\nManipulation, Sample-Efficient Learning\\nKnowledge Graph. Beyond identifying trending topics within individual research areas, an equally\\nimportant direction for discovery lies in uncovering cross-domain themes—topics that span multiple\\nfields and have the potential to catalyze interdisciplinary breakthroughs. In this work, we analyze the\\nintersections among four major domains: computer vision, natural language processing, machine learning,\\nand robotics. By examining these intersections, we aim to highlight not only where collaboration already\\nexists but also where it could be further cultivated.\\nAs shown in Figure below, the left side of the figure presents a Cross-Domain Topology Graph, where\\neach color corresponds to a specific research domain. Each node (represented as a sphere) signifies a\\ndistinct topic cluster derived from our embedding-based analysis, and edges between nodes indicate\\nsemantic or topical relationships—especially those that cross domain boundaries. Nodes located at the\\nperiphery, with few or no connecting edges, represent domain-specific topics that remain largely isolated\\n8'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='from other fields. In contrast, the densely connected regions at the center of the graph reflect genuinely\\ncross-domain topics, where ideas, methods, or applications from multiple fields converge. This view\\nempowers researchers to discover promising frontiers for collaboration, encourages rethinking isolated\\nproblems through new lenses, and supports a more forward-looking approach to scientific inquiry.\\nRetrieval Examples. Once a target research topic is identified, the next step is to pinpoint concrete entry\\npoints. We do this by leveraging the conference-level embeddings inferred earlier to run semantic searches\\nand retrieve the most relevant literature. For example, after surveying robotics, we focus on dexterous\\nmanipulation and query the embedding index to surface closely related papers across venues. As shown\\nin the table below, the returned papers align tightly with the query and exhibit meaningful community\\nimpact, as reflected by their venues, years, and citation counts.\\nPaper\\nYear\\nVenue\\nCitations\\nQuery: dexterous manipulation generated data in 3D simulation and evaluated in real world.\\nEvaluating Real-World Robot Manipulation Policies in Simulation\\n2024\\nCoRL24\\n127\\nLessons from Learning to Spin “Pens”\\n2024\\nCoRL24\\n29\\nGeneral In-hand Object Rotation with Vision and Touch\\n2023\\nCoRL23\\n134\\nTwisting Lids Off with Two Hands\\n2024\\nCoRL24\\n13\\nDexterityGen: Foundation Controller for Unprecedented Dexterity\\n2025\\nRSS25\\n16\\n5. Experiment\\nWe have presented a comprehensive qualitative analysis demonstrating how Real Deep Research supports\\ndeep dives into a chosen research focus. This section now details the dataset curated for our study and the\\nimplementation specifics required to realize the system. We then provide quantitative evaluations—both\\nby benchmarking our survey against commercial research tools and by validating the effectiveness of the\\nembeddings that underpin our approach.\\nVenue\\nYear\\nArea\\nTotal\\nCVPR\\n21-25\\nComputer Vision\\n11668\\nCoRL\\n21-24\\nRobotics\\n815\\nRSS\\n21-25\\nRobotics\\n575\\nICLR\\n21-25\\nMachine Learning\\n9549\\nACL\\n21-25\\nNLP\\n4556\\nNeurIPS\\n2024\\nMachine Learning\\n4240\\nECCV\\n2024\\nComputer Vision\\n6166\\nTable 1 | Paper Distribution Analysis across\\ndifferent venues, showing the total number of papers.\\nDataset. We curate our dataset from a collec-\\ntion of publicly available, high-impact confer-\\nence venues, focusing on those central to the\\nfields of artificial intelligence and robotics. As\\nshown in Table 1, the dataset includes papers from\\nvenues such as CVPR, ECCV (Computer Vision),\\nICLR, NeurIPS (Machine Learning), ACL (Nat-\\nural Language Processing), and CoRL and RSS\\n(Robotics). This selection allows us to capture\\na broad yet targeted view of the AI and robotics\\nresearch landscape from 2021 through 2025. To\\nalign with our focus on foundation models and\\n9'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='General\\nFoundation Model\\nRobotics\\nModel\\nRank\\nCV\\nNLP\\nML\\nRobotics\\nInput\\nModeling\\nOutput\\nSensor\\nBody\\nAction\\nGPT5\\n4.80\\n10.00\\n17.39\\n45.45\\n71.43\\n44.44\\n10.00\\n21.05\\n22.73\\n34.78\\n69.57\\nGPT5-Thinking\\n2.75\\n82.61\\n59.09\\n47.83\\n66.67\\n55.00\\n90.91\\n50.00\\n88.46\\n42.86\\n32.00\\nGPT5-Research\\n4.00\\n42.11\\n50.00\\n72.73\\n63.64\\n21.05\\n35.00\\n50.00\\n0.00\\n40.91\\n52.63\\nGemini\\n4.80\\n35.00\\n40.00\\n15.38\\n0.00\\n13.64\\n54.17\\n45.83\\n31.25\\n45.00\\n26.32\\nGemini-Thinking\\n3.35\\n63.64\\n50.00\\n56.25\\n37.50\\n65.22\\n45.45\\n41.67\\n55.56\\n56.52\\n34.78\\nRDR (Ours)\\n1.30\\n58.33\\n89.47\\n73.68\\n77.78\\n88.46\\n60.00\\n94.74\\n91.30\\n84.21\\n89.47\\nTable 2 | Survey Quality Evaluation among RDR and commercial based methods. We evaluate the\\npairwise winning rate for each domain and perspective.\\nrobotics, we apply an additional filtering step across all venues. Specifically, we identify and extract\\n4,424 papers related to foundation models and 1,186 papers focused on robotics, both from the year 2024\\nonward. This subset enables us to track the most recent developments in these fast-moving areas with\\nhigher resolution.\\nImplementation Details. We do not train any new networks in this work for generating the embedding or\\nsurvey; instead, we rely on off-the-shelf models. For straightforward tasks—such as classifying research\\nareas—we use the Doubao language model. For reasoning-intensive tasks and complex summarization,\\nwe employ the o3 model to achieve stronger performance. To extract text embeddings, we use nvidia/NV-\\nEmbed-v2.\\nSurvey Quality. As demonstrated in Sec. 3, our analysis of a research area begins with a broad survey\\nof the existing literature. The analysis pipeline we propose is designed to significantly reduce model\\nhallucination and produce a comprehensive, high-quality survey for a given research direction.\\nTo evaluate the accuracy and quality of the generated surveys, we conducted a user study involving\\nexperienced researchers with domain expertise in robotics and foundation models. As a baseline, we\\nprompted a commercial large language model using the following instruction: “Act as an expert research\\nanalyst. Your task is to create a structured map of the research landscape for a given academic or\\nindustrial field. The output must be a single, valid JSON object that categorizes the field into its primary\\nresearch areas and specific sub-topics. For the research area ’foundation model,’ can you summarize the\\ninput perspective with the following definition: The input processing for a foundation model generally\\ninvolves raw data and a tokenization procedure ...”\\nTo assess the quality of the generated surveys, we adopted a pairwise comparison methodology rather\\nthan asking evaluators to select a single best output. For each comparison, domain experts were presented\\nwith two survey outputs and asked to determine which one demonstrated superior quality and accuracy.\\nThis evaluation setup helps reduce cognitive load and bias, making the assessment more reliable by\\navoiding the need for evaluators to recall or rank multiple outputs simultaneously. In total, we collected\\n8 evaluation entries, each with 80 pairwise comparisons. To quantify performance, we computed the\\nwinning rate of each method within its respective domain.\\nAs shown in Tab. 2, our method, Real Deep Research (RDR), achieves the highest overall performance\\nwith an average rank of 1.30, outperforming all baselines. RDR leads in key domains such as NLP (89.47),\\nrobotics (77.78), and foundation model output (94.74), and also shows strong performance in robotics\\nsubfields like sensor (91.30) and action (89.47). While GPT5-Thinking slightly outperforms in CV (82.61)\\nand foundation model modeling (90.91), RDR consistently ranks at or near the top across nearly all\\ncategories.\\n10'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='Model\\nAG News\\n20 News Groups\\nACC(↑)\\nNMI(↑)\\nARI(↑)\\nACC(↑)\\nNMI(↑)\\nARI(↑)\\nLDA\\n74.05\\n47.17\\n49.01\\n29.05\\n31.63\\n13.34\\nNMF\\n34.05\\n4.59\\n2.13\\n12.42\\n12.86\\n0.48\\nProdLDA\\n80.93\\n56.51\\n60.91\\n37.42\\n45.67\\n23.89\\nDecTM\\n55.63\\n40.04\\n36.17\\n36.57\\n46.18\\n22.90\\nETM\\n26.14\\n0.00\\n0.00\\n5.35\\n0.10\\n0.00\\nNSTM\\n26.14\\n0.01\\n0.00\\n16.92\\n17.02\\n2.34\\nTSCTM\\n79.63\\n53.91\\n55.89\\n40.60\\n44.06\\n15.71\\nECRTM\\n78.69\\n54.05\\n54.88\\n25.70\\n31.00\\n12.26\\nBertopic\\n35.93\\n12.88\\n7.03\\n29.78\\n28.57\\n11.58\\nFASTopic\\n83.48\\n59.10\\n62.48\\n51.65\\n56.32\\n39.49\\nSciTopic*\\n85.29\\n61.96\\n65.94\\n70.88\\n68.32\\n55.71\\nRDR (Ours)\\n84.86\\n61.66\\n65.24\\n52.91\\n56.57\\n39.96\\nTable 3 | Unsupervised Clustering performance. *\\nindicate using more labels.\\nEmbedding Quality. Because much of our anal-\\nysis relies on high-quality embeddings, we evalu-\\nate their effectiveness using a simple linear probe\\ntrained on top of frozen representations—an ap-\\nproach that best reflects the intrinsic utility of\\nthe embeddings themselves. We follow the ex-\\nperimental protocol introduced in SciTopic [114],\\nusing the same unsupervised training and evalu-\\nation splits to ensure fair comparison. Unlike our\\nmethod, SciTopic uses pseudo-labels during train-\\ning, which introduces weak supervision; there-\\nfore, we gray out its entry in the results for clarity.\\nAs shown in Tab. 2, our method RDR achieves\\nthe best performance across both datasets, with\\nan accuracy of 84.86 on AG News and 52.91 on\\n20 News Groups. RDR also leads in NMI (61.66 and 56.57) and ARI (65.24 and 39.96), outperforming\\nall fully unsupervised baselines, and even surpassing the pseudo-supervised SciTopic model.\\nA. Appendix\\nThis appendix presents the complete results of our Real Deep Research (RDR) analysis across a wide\\nrange of domains. We include detailed domain-level surveys (e.g., AI, robotics, computer vision, natural\\nlanguage processing), perspective-based breakdowns (e.g., input/output modeling in foundation models,\\nsensor/action perspectives in robotics), and trend analyses to track the evolution of research focus over\\ntime. These results collectively offer a structured and insightful view of the research landscape, serving as\\na valuable reference for both new and experienced researchers.\\nDomain Survey\\n12\\nFoundation Model\\n12\\nRobotics\\n13\\nComputer Vision\\n14\\nNatural Language Processing\\n15\\nMachine Learning\\n16\\nNature\\n17\\nScience\\n19\\nPerspective Survey\\n20\\nFoundation Model Perspectives\\n20\\nRobotic Perspectives\\n25\\nTrend Analysis\\n30\\nComputer Vision\\n30\\nRobotics\\n31\\nNatural Language Processing\\n32\\nMachine Learning\\n33\\n11'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='Cat.\\nSub-category\\nWhat is covered\\nTypical examples\\nCluster\\n1. Model modalities & representations [32, 112, 195, 214, 230, 238, 241, 255, 272, 305]\\n1.1 Vision–Language\\n[30, 32, 225, 241, 300]\\nFoundation models that jointly process\\nimages/video and natural language.\\nVision-Language Models;\\nVision-Language Robotic FMs\\n0, 3\\n1.2 Multimodal (≥3)\\n[29, 204, 247, 295, 296]\\nArchitectures/objectives agnostic to the exact\\nmix of modalities.\\nMultimodal Foundation Models;\\nMultimodal LLMs; Multimodal Large\\nLanguage Models\\n1, 4, 5, 6\\n1.3 Open-vocabulary grounding\\n[24, 108, 131, 157, 268]\\nLinking free-form text to modality-specific\\nregions or anchors.\\nOpen-Vocabulary Grounding\\n1\\n1.4 3D/4D & video reps\\n[25, 68, 94, 252, 299]\\nLearned neural representations for 3-D/4-D\\nscenes and video.\\n3D & Multimodal Representation;\\nDiffusion-based 3D/4D Generation;\\n3D & Video Synthesis\\n0, 7, 9\\n1.5 Neural scene encoding\\n[53, 116, 163, 244, 297]\\nRepresentations enabling view-consistent\\nreconstruction.\\nMulti-view Consistent Reconstruction;\\nGaussian Splatting; NeRF\\nRepresentations\\n9\\n2. Generative & diffusion techniques [22, 31, 48, 130, 136, 212, 215, 243, 283, 284]\\n2.1 Core diffusion modelling\\n[118, 128, 138, 211, 283]\\nDiffusion processes used as the primary\\ngenerative backbone.\\nDiffusion Generative Modeling\\n7\\n2.2 Control & personalisation\\n[147, 164, 165, 174, 206]\\nSteering diffusion outputs with prompts,\\nadapters or user profiles.\\nControllable Diffusion Personalization;\\nControllable Efficient Sampling\\n7, 11\\n2.3 Robot policy via diffusion\\n[191, 232, 253, 283, 290]\\nUsing diffusion to learn control policies for\\nrobots or manipulators.\\nDiffusion-Based Policy Learning\\n3\\n2.4 Editing & post-generation\\n[15, 182, 189, 213, 223]\\nApplying diffusion to edit or refine existing\\ncontent.\\nDiffusion-based Generative Editing\\n4\\n2.5 Efficiency & distillation\\n[126, 184, 186, 211, 273]\\nSpeed-ups and compact student models for\\ndiffusion.\\nDiffusion Model Acceleration;\\nEfficient Sampling & Distillation\\n8\\n3. Training & adaptation strategies [8, 37, 40, 73, 79, 87, 96, 110, 129, 289]\\n3.1 Self-/pre-training paradigms\\n[1, 17, 41, 154, 218]\\nLarge-scale unsupervised or\\nweakly-supervised pre-training methods.\\nElastic Self-Supervised Pre-training\\n6\\n3.2 Prompt/adapter learning\\n[56, 110, 160, 197, 233]\\nLightweight modulation of frozen backbones\\nvia prompts or adapters.\\nPrompt/Adapter Tuning;\\nPrompt/Adapter Learning;\\nParameter-Efficient Prompt Tuning\\n0, 1, 5\\n3.3 Param-efficient finetune\\n[11, 72, 132, 159, 264]\\nLoRA/adapters that tune only a small slice of\\nparameters.\\nParameter-Efficient Fine-Tuning;\\nAdapter-Efficient Fine-Tuning\\n10, 11\\n3.4 Compression & inference\\nefficiency\\n[23, 50, 102, 169, 231]\\nSparsity, low-rank factorisation and runtime\\nacceleration.\\nSparse/Low-Rank Model\\nCompression; Efficient Transformer\\nInference\\n10\\n4. Safety, alignment & ethics [18, 65, 111, 142, 148, 151, 166, 240, 292, 304]\\n4.1 Safety alignment\\n[89, 259, 282, 291, 304]\\nAligning model behaviour with human or\\npolicy constraints.\\nLLM Safety Alignment; Alignment &\\nSafety\\n2, 6\\n4.2 Bias & harm mitigation\\n[7, 99, 143, 176, 188]\\nDetecting and reducing social or\\nrepresentational bias.\\nSafety & Bias Mitigation\\n11\\n4.3 Preference optimisation\\n[18, 166, 236, 240, 279]\\nFine-tuning with human preference or\\nRLHF-style signals.\\nPreference-Optimized Fine-Tuning\\n2\\n5. Embodied interaction & robotics [47, 60, 91, 92, 95, 139, 180, 226, 228, 265]\\n5.1 Robotic foundation models\\n[58, 91, 100, 170, 221]\\nGeneral-purpose models for perception and\\ncontrol on robots.\\nVision-Language Robotic Foundation\\nModels\\n3\\n5.2 Embodied agents\\n[33, 80, 90, 209, 262]\\nAgents acting in simulated or real\\nenvironments with multimodal inputs.\\nEmbodied Vision-Language Agents\\n4\\n5.3 Intended manipulation\\n[57, 84, 95, 113, 287]\\nGrounding natural-language instructions into\\nrobot actions.\\nMultimodal Instruction-Guided\\nManipulation\\n3\\n6. Reasoning & agent systems [86, 90, 137, 181, 205, 207, 209, 210, 270, 274]\\n6.1 Multi-agent reasoning\\n[90, 207, 209, 260, 274]\\nCoordinated planning or dialogue among\\nseveral learned agents.\\nMulti-Agent Reasoning\\n2\\n7. Generalisation & robustness [5, 21, 38, 45, 63, 66, 101, 172, 192, 285]\\n7.1 Domain robustness\\n[21, 203, 228, 251]\\nTechniques to maintain performance under\\ndomain shift.\\nDomain-Robust Generalization\\n5\\nTable 4 | Domain Survey for Foundation Model.\\n12'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='Cat.\\nSub-category\\nWhat is covered\\nTypical examples\\nCluster\\n1. Perception & Mapping [71, 168, 185, 277, 294]\\n1.1 Multimodal sensor fusion\\n[14, 123, 224, 256, 303]\\nFuse heterogeneous sensors for\\nricher scene understanding\\nLiDAR–Camera Fusion; Radar–Camera Fusion;\\nV2X Cooperative Perception ...\\n0,6,7,8,14,16\\n1.2 3D reconstruct/occupancy\\n[85, 133, 140, 175, 294]\\nBuild dense or sparse geometric\\nmaps for localisation\\n3-D SLAM & Reconstruction; 3-D Occupancy;\\nEfficient 3-D Representation\\n0,8,16\\n1.3 BEV / top-view mapping\\n[19, 93, 120, 256, 286]\\nBird’s-eye or top-down\\nrepresentations for planning\\nBEV Perception; V2X Collaborative Perception\\n0,14,16\\n2. Manipulation & Grasping [13, 95, 208, 216, 239]\\n2.1 Dexterous grasping\\n[146, 216, 249, 254, 271]\\nMulti-finger in-hand manipulation\\nDexterous Robotic Grasping; Dexterous Grasp &\\nManipulation\\n11,12\\n2.2 Generalist manipulation\\n[36, 58, 125, 266, 280]\\nSingle policy handles diverse\\nobjects/tasks\\nGeneralist Robotic Manipulation; Robotic\\nManipulation; Humanoid Manipulation\\n3,4,9\\n2.3 Tactile-vision fusion\\n[51, 82, 248, 261]\\nCombine touch and vision for\\nreactive grasps\\nMultimodal Tactile-Vision Learning\\n11\\n3. Locomotion & Navigation [34, 74, 75, 135, 239]\\n3.1 Legged locomotion control\\n[61, 75, 135, 250, 302]\\nWhole-body control and adaptation\\non uneven terrain\\nLegged Robot Locomotion; Learning-Based\\nControl & Adaptation\\n17\\n3.2 Embodied VL navigation\\n[104, 179, 220, 276]\\nLanguage-directed navigation with\\nactive mapping\\nEmbodied Vision-Language Navigation; Active\\n3-D Mapping & Planning\\n7,19\\n4. Planning & Control [54, 91, 107, 293, 301]\\n4.1 Language/hierarchical\\nplanning [10, 69, 119, 194]\\nTranslate language or high-level\\ngoals into executable skills\\nLanguage-Guided Planning & Control;\\nHierarchical Skill Planning & Adaptation\\n2,18\\n4.2 Diffusion/Transformer\\npolicies [98, 127, 152, 173, 191]\\nTrajectory generation with\\ngenerative sequence models\\nDiffusion Policies; Diffusion/Transformer Policy\\nModels; Generative Diffusion Models\\n1,9,14\\n5. Robot Learning & Adaptation [54, 88, 91, 107, 301]\\n5.1 RL & imitation\\n[52, 78, 109, 258, 275]\\nLearn skills from rewards,\\ndemonstrations or offline data\\nRobot Reinforcement Learning; Imitation\\nLearning Policies; Sample-Efficient RL ...\\n3,9,15\\n5.2 Sim to real & continual\\nadaptation\\n[39, 59, 103, 150, 257]\\nTransfer and improve policies\\nacross domains over time\\nContinual Sim-to-Real Adaptation; Sim-to-Real\\nTransfer; Self-Supervised Distillation/Adaptation\\n0,4,15,16\\n5.3 Multitask / generalisable\\npolicies [46, 158, 221, 263]\\nSingle policy generalises across\\nmany tasks and embodiments\\nMultitask Generalisable Robotics\\n1\\n6. Autonomous Driving [77, 185, 201, 224, 227]\\n6.1 Motion forecasting,\\nperception & simulation\\n[77, 167, 227, 237]\\nForecast traffic actors, all-weather\\nperception, long-tail scenario\\nsimulation\\nMotion Forecasting; Trajectory Prediction;\\nDriving Perception; Scenario Simulation\\n5,6,8,13,14\\n7. Simulation & World Models [27, 64, 117, 227, 229]\\n7.1 Generative world models\\n[9, 70, 81, 227, 278]\\nLearn latent physics/world models\\nfor planning or RL\\nGenerative World Models\\n12\\n7.2 Self-supervised simulation\\n[4, 16, 49, 121, 178]\\nExpand synthetic experience using\\nself-supervised signals\\nSelf-Supervised Generative Simulation\\n5\\n8. Embodied Language Robotics [10, 179, 183, 235]\\n8.1 LLM-driven robotics\\n[26, 42, 122, 202]\\nUse large language models for\\nzero-shot policy/reasoning\\nLLM-Driven Robotics; LLM-Enhanced Driving;\\nLLM-Driven Zero-Shot Planning\\n2,13,19\\n8.2 Vision-language control\\n[10, 44, 179, 265, 287]\\nPair vision with text to drive\\nlow-level actions\\nVision-Language Robotic Control; Hierarchical\\nSkill Planning & Adaptation\\n18\\n8.3 Open-vocabulary mapping\\n[28, 179, 183, 228, 235]\\nBuild scene maps labelled with\\nfree-form language\\nOpen-Vocabulary Scene Mapping\\n19\\n9. Safety & Robustness [75, 95, 185, 187, 291]\\n9.1 Safety-aware planning\\n[43, 67, 75, 162, 185]\\nExplicit risk reasoning during\\nmotion generation\\nSafety-Aware Motion Planning\\n10\\n9.2 Runtime monitoring\\n[2, 134, 185, 198, 199]\\nDetect and mitigate failures\\non-the-fly\\nFailure Detection & Runtime Monitoring\\n18\\n9.3 Robust control\\n[144, 171, 193, 200, 306]\\nImprove stability against\\ndisturbances and uncertainties\\nSafety & Robustness (Locomotion)\\n17\\n10. Multi-Robot & Human Collaboration [35, 36, 54, 74, 90]\\n10.1 Multi-agent collaboration\\n[144, 171, 193, 200, 306]\\nPlan and act with other robots or\\nhumans in the loop\\nMulti-Agent / Human-Robot Collaboration\\n10\\nTable 5 | Domain Survey for Robotics.\\n13'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='Category\\nSub-category\\nWhat is covered\\nTypical examples\\nCluster\\n1. Robust & Generalizable Learning\\n1.1 Adversarial / OOD\\nRobustness\\nDefending against or detecting malicious,\\nanomalous or out-of-distribution inputs.\\nadversarial robustness; deepfake\\ndetection; anomaly detection;\\nout-of-distribution detection\\n0,2,5\\n1.2 Domain Adaptation &\\nGeneralization\\nTransferring models across different\\ndomains, devices or persons without\\nperformance drop.\\ndomain adaptation; domain\\ngeneralization; test-time adaptation;\\nperson re-identification\\n3,6\\n1.3 Low-Data Learning\\nLearning reliably from scarce,\\nimbalanced or continually arriving data.\\nfew-shot; continual learning; long-tailed\\nrecognition; federated learning\\n0,1,2\\n2. Representation & Model Efficiency\\n2.1 Representation\\nLearning & Distillation\\nUn/semisupervised learning and\\ndistillation techniques that build\\ninformative, explainable features.\\nself-supervised learning;\\nsemi-supervised segmentation;\\npseudo-label consistency; representation\\nlearning; knowledge distillation;\\nexplainability\\n4,5,7,10\\n2.2 Efficient\\nArchitectures\\nDesigning compact, hardware-friendly or\\nautomatically searched neural networks.\\nefficient ViT; neural architecture\\ncompression; sparse/quantized NAS\\n9\\n3. Generative Modeling & Editing\\n3.1 2D Generative\\nImaging\\nSynthesising or editing images/videos\\nwith controllable appearance or\\ncompression.\\ngenerative adversarial networks; image\\ninpainting; image translation; neural\\nstyle transfer; diffusion-based\\nimage/video generation; controllable\\ngenerative editing; neural compression\\n18,21,23\\n3.2 3D Neural Rendering\\n& Scene Generation\\nGenerating or reconstructing 3-D scenes\\nvia implicit or explicit neural\\nrepresentations.\\nneural radiance fields; 3D scene\\ngeneration; 3DGS; dynamic scene\\nreconstruction; neural rendering\\n25,28,29\\n4. 2D Perception & Enhancement\\n4.1 Detection &\\nSegmentation\\nLocating objects or semantic regions in\\nimages/videos.\\nobject detection; semantic segmentation;\\nfew-shot detection\\n1,10,16\\n4.2 Tracking & Motion\\nEstimation\\nFollowing objects or estimating pixel\\ncorrespondences over time.\\nobject tracking; correspondence;\\nregistration; optical flow; UAV\\n13,20\\n4.3 Matting &\\nTransparency\\nSeparating foreground layers or\\ntransparency effects in images/videos.\\nimage/video matting; trimap guidance;\\nmask guidance; transformer-based\\nmatting models\\n19\\n4.4 Restoration &\\nEnhancement\\nImproving quality of degraded\\nimages/videos or reconstructing HDR.\\nimage/video restoration; diffusion\\nmodels for restoration; HDR\\n21,27\\n5. 3D Perception & Geometry\\n5.1 Depth &\\nReconstruction\\nEstimating depth or reconstructing 3-D\\nstructure from images.\\ndepth estimation; stereo matching; 3D\\nreconstruction\\n26\\n5.2 LiDAR & 3D\\nDetection\\nUnderstanding point clouds for object\\ndetection and semantic segmentation.\\nLiDAR point clouds; 3D object\\ndetection; 3D semantic segmentation\\n16\\n5.3 Pose & Localization\\nEstimating 6-D object poses or\\nlocalizing cameras in space.\\n6D pose estimation; visual localization;\\nequivariant features\\n24\\n6. Video & Temporal Understanding\\n6.1 Temporal Action &\\nVideo Reasoning\\nRecognising and localising actions or\\nreasoning over temporal video cues.\\ntemporal action localization; video\\nrepresentation; multimodal reasoning\\n12\\n7. Multimodal & Vision-Language Systems\\n7.1 Vision-Language\\nPretraining & Retrieval\\nLearning cross-modal representations for\\nzero-shot tasks or retrieval.\\nvision-language pretraining; zero-shot\\nlearning; cross-modal retrieval\\n8\\n7.2 Multimodal Large\\nModels & Grounding\\nLarge models that jointly reason over\\nvision and language with grounding.\\nmultimodal large language models;\\nvisual grounding; visual reasoning;\\nbenchmark datasets; 3D vision-language\\ngrounding; scene graph generation\\n11,14\\n7.3 Audio / Sign / Gaze\\nMultimodality\\nIntegrating audio, sign language or gaze\\nwith vision tasks.\\naudio-visual learning; sign language\\nprocessing; gaze estimation\\n15\\n8. Human-Centric Understanding & Animation\\n8.1 Pose & Interaction\\nEstimating human body pose and\\nmodelling human-object interactions.\\n3D human pose estimation;\\nhuman-object interaction;\\ntransformer-based motion generation\\n22\\n8.2 Avatars & Animation\\nBuilding and animating realistic 3-D\\nhuman avatars.\\n3D human avatars; pose-driven\\nanimation; neural rendering of humans\\n28\\n9. Embodied & Autonomous Systems\\n9.1 Embodied Navigation\\n& Mapping\\nPerception and planning for agents\\nnavigating 3-D environments.\\nembodied navigation; HD-map\\ngeneration; lane generation\\n14,17\\n9.2 Trajectory Prediction\\n& Traffic Simulation\\nForecasting future paths and simulating\\nrealistic traffic participants.\\ntrajectory prediction; data-driven traffic\\nsimulation\\n17\\nTable 6 | Domain Survey for Computer Vision.\\n14'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='Category\\nSub-category\\nWhat is covered\\nTypical examples\\nCluster\\n1. Information Extraction\\n1.1 Entity & Relation\\nExtraction\\nAutomatic detection of named entities plus the\\nsemantic relations or events connecting them.\\nNamed Entity Recognition, Relation\\nExtraction, Event Extraction\\n0\\n2. Text Generation & Summarization\\n2.1 Summarization &\\nKeyphrase Generation\\nProducing concise summaries or keyphrases from\\nlonger documents.\\nSummarization, Keyphrase,\\nEvaluation\\n12\\n2.2 Controllable &\\nStylistic Generation\\nGenerating text under user-specified style or\\nattribute constraints.\\nStyle transfer, controllable text\\ngeneration, representations\\n27\\n3. Dialogue & Conversational Systems\\n3.1 Task-oriented\\nDialogue\\nDialogue systems that track state and generate\\nresponses to accomplish user goals.\\nDialogue systems, Response\\ngeneration, Dialogue state tracking\\n14\\n3.2 Empathetic & Safe\\nDialogue\\nHandling empathy, hate speech and multimodal\\ncues in conversations.\\nHateSpeechDetection,\\nEmpatheticDialogue, ...\\n25\\n4. Multilingual & Cross-lingual NLP\\n4.1 Multilingual\\nModeling & Transfer\\nBuilding models that operate across many (often\\nlow-resource) languages and transfer knowledge\\nbetween them.\\nlow-resource languages, multilingual\\nlanguage models, cross-lingual\\ntransfer\\n16\\n4.2 Multilingual Machine\\nTranslation\\nNeural translation among multiple language pairs,\\noften using shared or distilled models.\\nNeural machine, Knowledge distill,\\nMultilingual modeling\\n19\\n4.3 Multimodal\\nLow-Resource Speech\\nSpeech translation/recognition when data are\\nscarce or involve multiple modalities.\\nspeech translation, multimodal\\nlearning, low-resource speech\\n15\\n5. Knowledge & Reasoning\\n5.1 Knowledge Graph\\nReasoning\\nEmbedding and temporal/causal reasoning over\\nstructured knowledge graphs.\\nknowledge graph embedding, event\\ncausality reasoning, temporal\\nknowledge reasoning\\n1\\n5.2 Mathematical &\\nChain-of-Thought\\nReasoning\\nUsing large language models for step-by-step\\nlogical or mathematical reasoning.\\nLarge language models,\\nMathematical reasoning,\\nChain-of-thought prompting\\n10\\n5.3 Compositional &\\nSyntactic Generalization\\nProbing or improving models to generalize\\ncompositionally or parse syntax.\\nsyntactic parsing, compositional\\ngeneralization, lan. model probing\\n3\\n6. Retrieval & Question Answering\\n6.1 Dense Retrieval &\\nRAG\\nLearning dense vector search for open-domain\\nQA and retrieval-augmented generation.\\nDense retrieval, open-domain\\nquestion answering,\\nretrieval-augmented generation\\n4\\n6.2 Table & Structured\\nQA / Generation\\nMapping natural language to SQL, answering\\nqueries or generating text from structured data.\\nText-to-SQL, Table Question\\nAnswering, Data-to-Text Generation\\n2\\n7. Evaluation, Alignment & Editing\\n7.1 LLM Evaluation &\\nHuman Alignment\\nDesigning metrics and feedback loops to align\\nlarge language models with human intent.\\nLLM evaluation, alignment methods,\\nhuman feedback\\n23\\n7.2 Hallucination,\\nCalibration & Knowledge\\nEditing\\nDetecting/mitigating false outputs and editing or\\ncalibrating model knowledge.\\nhallucination, knowledge editing,\\ncalibration\\n11\\n7.3 Evaluation Metrics &\\nData Augmentation\\nDeveloping metrics and synthetic data (incl.\\nfigurative language) to assess or improve models.\\nEvaluation metrics, Data\\naugmentation, Figurative language\\n13\\n8. Model Training Paradigms & Efficiency\\n8.1 Continual, In-context\\n& Instruction Tuning\\nAllowing models to learn new tasks or follow\\ninstructions without full retraining.\\ncontinual learning, instruction tuning,\\nin-context learning\\n17\\n8.2 Parameter-Efficient &\\nCompressed Models\\nReducing training/inference cost via adapters,\\npruning or lightweight fine-tuning.\\nparameter-efficient fine-tuning,\\nmodel compression for LLMs\\n20\\n8.3 Transformer\\nEfficiency &\\nLong-Context Modeling\\nArchitectural or computational methods to scale\\ntransformers to longer contexts efficiently.\\nTransformer efficiency, long-context\\nmodeling, adaptive computation\\n7\\n8.4 Sentence &\\nMultilingual\\nRepresentation Learning\\nContrastive or related methods to build versatile\\nsentence embeddings across languages.\\nmultilingual representation learning,\\nsentence embeddings, contrastive\\nlearning\\n6\\n9. Safety, Bias & Robustness\\n9.1 Social Bias &\\nFairness\\nMeasuring and mitigating demographic or social\\nbiases in NLP systems.\\nsocial bias, debiasing, fairness\\nevaluation\\n26\\n9.2 Misinformation &\\nFact Verification\\nDetecting false claims, AI-generated text or\\naligning model values with truthfulness.\\nfact verification, misinformation\\ndetection, evidence retrieval, fake\\ndetection, value alignment\\n18,28\\n9.3 Security & Privacy\\nRobustness\\nProtecting models against adversarial, backdoor\\nor privacy attacks.\\nadversarial robustness, backdoor\\nattacks, privacy preservation\\n29\\n10. Agents & Interactive Reasoning\\n10.1 LLM-based Agents\\n& Planning\\nUsing large language models as autonomous\\nagents capable of interactive planning and\\ntheory-of-mind reasoning.\\nLLM agents, interactive planning,\\ntheory of mind\\n21\\n11. Code Intelligence\\n11.1 Code Generation &\\nBenchmarks\\nGenerating executable code and evaluating\\nmodels on coding tasks.\\ncode generation, large language\\nmodels, benchmark evaluation\\n24\\nTable 7 | Domain Survey for Natural Language Processing.\\n15'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='Category\\nSub-category\\nWhat is covered\\nTypical examples\\nCluster\\n1. Generative Modelling & Media Synthesis\\n1.1 Image / Video\\nGeneration & Editing\\nModels that create or edit 2-D or temporal\\nvisual content via generative techniques.\\nGenerative modeling; Image\\nsynthesis/editing; Diffusion-based\\nmethods; Optimal Transport; Diffusion\\nModels\\n1, 3\\n1.2 3-D Object &\\nMolecule Generation\\nGenerating 3-D shapes or molecular\\nstructures using geometry-aware or\\nequivariant models.\\n3D shape generation; neural implicit\\nrepresentations; point cloud reconstruction;\\nequivariant GNNs; 3D molecular\\ngeneration; drug discovery\\n9, 16\\n1.3 Audio & Speech\\nSynthesis\\nProducing speech or audio from text or\\nmultimodal cues, via diffusion models.\\ntext-to-speech; audio-visual; diffusion\\n2\\n2. Representation & Transfer Learning\\n2.1 Continual / Few-Shot\\n/ Domain Adaptation\\nAdapting models continually, with few\\nexamples, or across shifting domains.\\nContinual Learning; Few-Shot Learning;\\nDomain Adaptation\\n0\\n2.2 Self-, Contrastive &\\nRetrieval-Augmented\\nLearning\\nBuilding rich representations via\\nself/contrastive learning or external retrieval\\naugmentation.\\nSelf-supervised Learning; contrastive\\nlearning; disentangled representations;\\nclustering; language-modeling;\\nretrieval-augmentation;\\nrepresentation-learning\\n5, 6,\\n15\\n2.3 Parameter-Efficient\\nTransfer\\nAdapting large transformers with minimal\\nnew parameters and compute.\\nEfficient-transformer-architectures;\\nparameter-efficient-fine-tuning;\\nmultilingual-adaptation\\n7\\n3. Robustness, Security & Privacy\\n3.1 Adversarial &\\nBackdoor Robustness\\nDefending against adversarial or backdoor\\nmanipulations and distribution shifts.\\nadversarial robustness; backdoor attacks;\\nrobustness; knowledge distillation;\\ndistribution shift\\n10, 4\\n3.2 Uncertainty &\\nInterpretability\\nQuantifying model confidence and\\nexplaining predictions.\\nuncertainty estimation; conformal\\nprediction; model interpretability\\n14\\n3.3 Privacy & Machine\\nUnlearning\\nEnsuring data privacy and enabling deletion\\nor secure distributed learning.\\ndifferential privacy; machine unlearning;\\nfederated learning; robust optimization\\n19, 24\\n4. Model Efficiency & Compression\\n4.1 Pruning, Quantization\\n& Embedding\\nCompression\\nCompressing networks by pruning,\\nquantizing or embedding reduction for\\nefficient deployment.\\nNetwork pruning;\\nSparse_Network_Pruning; Low-precision\\nquantization; Embedding_Compression;\\nEfficient architecture search;\\nRecommendation_Systems\\n8, 21\\n5. Geometric & Graph Learning\\n5.1 Equivariant /\\nGeometric Deep\\nNetworks\\nNetworks that respect group symmetries to\\nlearn geometric or physical structures.\\nequivariant neural networks; group\\nsymmetry; geometric deep learning\\n12\\n5.2 Graph Neural\\nNetwork Theory\\nTheoretical properties, expressivity and\\nrobustness of Graph Neural Networks.\\nGraph Neural Networks; Expressivity;\\nRobustness\\n20\\n6. Optimization & Theory\\n6.1 Non-convex &\\nStochastic Optimization\\nAlgorithms and analysis for nonconvex\\noptimization with stochastic gradients.\\nNonconvex optimization; Stochastic\\ngradient methods; Convergence analysis\\n18\\n6.2 Neural Network\\nTheory & Neuroscience\\nInspiration\\nTheoretical studies and neuro-inspired\\nmodeling of recurrent nets.\\nrecurrent neural networks;\\nneuroscience-inspired modeling;\\ntheoretical analysis\\n17\\n7. Reinforcement Learning & Embodied Intelligence\\n7.1 Core & Offline\\nReinforcement Learning\\nImproving sample efficiency and offline\\npolicy learning in RL.\\nReinforcement Learning; Offline Learning;\\nSample Efficiency\\n29\\n7.2 Multi-Agent &\\nDialogue RL\\nLearning cooperation, competition or\\ndialogue among multiple agents.\\nmulti-agent reinforcement learning; bandit\\nalgorithms; game-theoretic learning;\\ndialogue systems; multi-agent\\ncollaboration; reinforcement learning\\n28, 26\\n7.3 Embodied AI &\\nRobotics\\nTraining embodied agents and robots via\\ndifferentiable simulation and manipulation\\ntasks.\\nEmbodied AI; Robotic manipulation;\\nDifferentiable simulation\\n27\\n8. Multimodal Perception & Reasoning\\n8.1 Vision-Language &\\nKnowledge Reasoning\\nJoint reasoning across vision and language\\nplus knowledge graphs.\\nvision-language reasoning; knowledge\\ngraph learning; compositional\\ngeneralization\\n11\\n8.2 Video Understanding\\n& 3-D Perception\\nTemporal and 3-D understanding of videos\\nand dynamic scenes.\\nVideo understanding; Temporal modeling;\\n3D perception\\n13\\n9. Scientific & Symbolic Machine Learning\\n9.1 Physics & Differential\\nEquation-guided\\nLearning\\nLearning operators governed by physical\\nlaws and differential equations.\\nNeural differential equations;\\nPhysics-informed operator learning;\\nSpatiotemporal forecasting\\n23\\n9.2 Program Synthesis &\\nAutomated Reasoning\\nAutomatically generating code or formal\\nproofs from specifications.\\nProgram synthesis; Code generation;\\nTheorem proving\\n22\\n9.3 Combinatorial,\\nCausal & Bayesian\\nOptimization\\nOptimization over discrete structures,\\ncausal questions or Bayesian objectives.\\nCombinatorial optimization; Causal\\ninference; Bayesian optimization\\n25\\nTable 8 | Domain Survey for Machine Learning.\\n16'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='Category\\nSub-category\\nWhat is covered\\nTypical examples\\nCluster\\n1. Life Sciences & Biomedicine\\n1.1 Immuno-oncology &\\nMetabolic Signalling\\nImmune mechanisms in cancer and\\nmetabolic cues that modulate them\\nT-cell immunity; tumour\\nmicroenvironment; metabolic signalling\\n0\\n1.2 Cancer Genomics &\\nEpigenetics\\nGenetic and epigenetic alterations driving\\noncogenesis and therapy response\\nCancer; DNA repair; epigenetics\\n1\\n1.3 Infectious Disease &\\nMicrobiome Interactions\\nHost–pathogen dynamics and microbiome\\necology shaping antimicrobial strategies\\nHost–pathogen interactions;\\nantimicrobial therapeutics; microbiome\\ndynamics\\n2\\n1.4 Neuro-immune\\nMetabolism & Aging\\nCrosstalk between immune system,\\nmetabolism and brain across aging\\nNeuroimmunology; metabolism; aging\\n3\\n1.5 Genome Editing &\\nMicrobial/Plant Immunity\\nEngineering genomes and decoding\\nmicrobial\\nplant defence mechanisms &\\nCRISPR-based genome editing;\\nbacterial anti-phage defence; plant\\nimmune signalling\\n4\\n1.6 Protein & RNA\\nEngineering\\nDesigning proteins and regulating\\nchromatin/RNA to control cell function\\nProtein design; chromatin regulation;\\nRNA processing\\n5\\n1.7 Neural Epigenetics &\\nDisorders\\nEpigenetic regulation of neural plasticity\\nand neuropsychiatric disease\\nNeural circuit plasticity; epigenetic\\nregulation; neuropsychiatric disorders\\n6\\n1.8 Population &\\nSingle-Cell Genomics\\nSequencing-based mapping of genetic\\nvariation at population & cellular\\nresolution\\nGenome sequencing; population\\ngenetics; single-cell transcriptomics\\n7\\n1.9 Connectomics &\\nBehaviour\\nStructural mapping of neural circuits to\\nexplain behaviour\\nConnectomics; neural circuit mapping;\\nbehaviour\\n8\\n1.10 Evolutionary Genomics\\n& Paleobiology\\nReconstructing evolutionary history using\\nancient DNA and fossils\\nPaleogenomics; prehistoric migrations;\\nfossil record\\n9\\n2. Chemistry & Materials Science\\n2.1 Catalysis & Green\\nSynthesis\\nCatalytic and synthetic routes for\\nsustainable chemical production\\nAdvanced catalysis; sustainable\\nchemistry; synthetic methodologies\\n10\\n2.2 Functional Materials for\\nEnergy & Electronics\\nMultifunctional materials for energy\\nstorage and flexible devices\\nAdvanced materials; energy storage;\\nflexible electronics\\n12\\n2.3 Perovskite Solar\\nTechnologies\\nHigh-efficiency perovskite and tandem\\nphotovoltaics with interface engineering\\nPerovskite photovoltaics; tandem solar\\ncells; interface passivation\\n13\\n2.4 Integrated Photonics &\\nOptoelectronic Integration\\nIntegration of perovskites and 2D\\nsemiconductors into photonic platforms\\nIntegrated photonics; perovskite\\noptoelectronics; 2D semiconductor\\nintegration\\n14\\n3. Physics & Quantum Technology\\n3.1 Quantum Materials\\nEmergent quantum phases in topological\\nand moiré systems, incl. unconventional\\nsuperconductivity\\nTopological quantum matter; moiré\\nheterostructures; unconventional\\nsuperconductivity\\n16\\n3.2 Quantum Computing\\nHardware & Networks\\nScalable, fault-tolerant quantum\\nprocessors and quantum communication\\nlinks\\nFault-tolerant quantum computing;\\nscalable qubit hardware; quantum\\nnetworking\\n17\\n4. Earth & Environmental Science\\n4.1 Climate Change &\\nEcosystem Impacts\\nHow climate change alters ecosystems and\\nthe global environment\\nClimate-change; ecosystem-impacts;\\nglobal-environment\\n15\\n5. Astronomy & Astrophysics\\n5.1 Exoplanetary Science\\nwith JWST\\nCharacterising exoplanet atmospheres and\\ninteriors using JWST observations\\nExoplanet atmospheres; planetary\\ninteriors; JWST observations\\n18\\n5.2 Early-Universe &\\nBlack-Hole Astronomy\\nGalaxy formation and supermassive black\\nholes in the early Universe probed with\\nJWST\\nJWST; early-Universe galaxies;\\nsupermassive black holes\\n19\\n6. Computer Science & Artificial Intelligence\\n6.1 Foundation &\\nTrustworthy AI\\nLarge foundation models, applied AI and\\nmethods ensuring fairness & reliability\\nFoundation models; applied artificial\\nintelligence; fairness and reliability\\n11\\nTable 9 | Domain Survey for Natural related Topics.\\n17'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='Category\\nSub-category\\nWhat is covered\\nTypical examples\\nCluster\\n1. Earth & Environmental Sciences\\n1.1 Climate & Ecosystem\\nDynamics\\nInteractions among climate\\nchange, carbon cycling and\\nbiodiversity, including\\nconservation responses\\nBiodiversity loss; Conservation strategies;\\nClimate change impacts; Climate change;\\nCarbon cycle; Environmental impacts\\n0,1\\n1.2 Geophysical\\nProcesses\\nPhysical processes shaping\\nEarth’s solid and cryospheric\\nsystems\\nEarthquakes; Volcanism; Ice dynamics\\n2\\n2. Space Science\\n2.1 Stellar &\\nSpace-Plasma Physics\\nPhysics of stars, solar activity\\nand the interstellar medium\\nCompact objects; Interstellar medium;\\nSolar activity\\n9\\n3. Biological Sciences\\n3.1 Evolutionary\\nGenomics\\nGenetic mechanisms driving\\nadaptation and speciation\\nEvolutionary genomics; adaptation;\\nspeciation\\n3\\n3.2 Molecular & Cellular\\nRegulation\\nMolecular signaling and\\nstructural mechanisms\\ngoverning development and\\ngenome integrity\\nHormone signaling; Immune defense;\\nDevelopmental regulation; Genome\\nstability; Chromosome segregation;\\nCryo-EM structural biology\\n4,5\\n3.3 Neurobiology &\\nSystems Neuroscience\\nGene-to-circuit bases of neural\\nfunction, plasticity and\\nbehaviour\\nNeuroscience; Gene regulation;\\nSingle-cell; neural circuits; synaptic\\nplasticity; behavior\\n6,8\\n3.4 Immunity, Infection &\\nTherapy\\nHost defence mechanisms and\\nengineered immunotherapies\\nagainst pathogens and cancer\\nAntiphage immunity; CRISPR systems;\\nAntibiotic discovery; Immunoregulation;\\nMetabolic signaling; Cancer therapy;\\nInfectious disease; Immunotherapy;\\nMolecular engineering\\n7,11,13\\n3.5 Synthetic &\\nComputational Biology\\nDesign of biomolecules and\\nbiological systems using AI and\\nsynthetic methods\\nprotein-design; deep-learning;\\nsynthetic-biology\\n12\\n4. Materials & Chemical Sciences\\n4.1 Catalysis & Chemical\\nTransformations\\nSelective catalytic methods for\\nconstructing organic molecules\\nCatalytic organic synthesis;\\nRadical-mediated transformations;\\nSelective C–H functionalization\\n14\\n4.2 Advanced Functional\\nMaterials\\nSmart, biointegrated and\\nnanostructured materials with\\ntailored properties\\nSmart materials; Biointegrated\\nelectronics; Soft robotics; Nanostructured\\nmaterials; Energy storage; Functional\\nproperties\\n15,17\\n4.3 Energy Conversion &\\nSeparation Materials\\nMaterials enabling\\nelectrochemical, thermal and\\nmembrane-based energy\\ntechnologies\\nElectrocatalysis; Porous framework\\nmaterials; Membrane separations;\\nPerovskite photovoltaics; Thermoelectric\\ndevices; Radiative cooling\\n16,18\\n5. Physics & Quantum Technologies\\n5.1 Quantum Materials &\\nInformation\\nExotic quantum phases and their\\napplication to information\\nprocessing\\nTopological quantum phases; Quantum\\ninformation processing; Strongly\\ncorrelated matter\\n19\\n6. Computational & Social Systems Science\\n6.1 Information\\nDynamics & Society\\nComputational study of\\ninformation spread and\\npersuasion in sociotechnical\\nsystems\\nmisinformation propagation; democratic\\npolarization; AI-mediated persuasion\\n10\\nTable 10 | Structured overview of clustered science research areas.\\n18'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='Category\\nSub-category\\nWhat is covered\\nTypical examples\\nCluster\\n1. Pharmacogenomics & Genetics-Guided Therapy\\n1.1 Cytochrome P450\\nGenotype-Driven\\nTherapy\\nLinks CYP450 genetic variants to\\ndrug exposure and response for\\nindividualised dosing.\\nCYP2C19 pharmacogenetics,\\nstar-allele variability, precision\\nantithrombotic therapy,\\nantidepressant pharmacogenetics\\n1,3,4\\n1.2 Transporter\\nPharmacogenetics\\nExamines genetic variation in drug\\ntransporters and its impact on safety\\nand efficacy.\\nSLCO1B1 variants, statin-associated\\nmuscle symptoms\\n6\\n1.3 PGx Implementation\\n& Economic Evaluation\\nAssesses clinical decision support,\\nworkflow integration, and the\\ncost-effectiveness of\\npharmacogenomics.\\npharmacogenomics implementation,\\nclinical decision support,\\ncost-effectiveness\\n11\\n1.4 Oncology / High-Risk\\nTherapy PGx\\nUses germline variants to predict\\ntoxicity and guide dosing of\\nhigh-risk or anticancer drugs.\\nDPYD variants, TPMT variants,\\nNUDT15 variants, chemotherapy\\ntoxicity prediction,\\npharmacogenetic-guided dosing\\n21\\n2. Quantitative Pharmacology & Model-Informed Drug Development\\n2.1 Population PK &\\nDose Optimisation\\nApplies population PK and\\nexposure–response models to refine\\ndosing in diverse patients.\\npopulation pharmacokinetics,\\nprecision dosing, anticoagulants,\\nexposure–response modelling,\\noncology real-world evidence\\n2,17,25,29\\n2.2 Mechanistic PBPK &\\nSpecial Populations\\nUses physiologically based PK\\nmodels to predict drug disposition in\\npaediatrics, the CNS, pregnancy, and\\nother special populations.\\npaediatric PBPK modelling, CNS\\ndrug delivery, maternal–infant\\npharmacology, anti-infective therapy\\n7,19\\n2.3 Exposure–Response\\nfor Biologic / Cell\\nTherapies\\nCharacterises PK/PD and\\ndose–response of biologics and\\ncell-based therapies.\\nPK/PD modelling, haematological\\ntherapies, biologic PK/PD,\\nimmunomodulatory therapies, T-cell\\nengagers\\n15,26,28\\n2.4 Machine-Learning-\\nAssisted Precision Dosing\\nIntegrates machine learning with PK\\nmodels and real-world factors to\\nindividualise therapy.\\nmachine learning, precision\\npharmacokinetic modelling,\\ntransplant immunosuppressant\\ndosing, gut microbiota influence,\\nmodel-informed drug development\\n18,20\\n3. Drug Metabolism, Transport & Interaction Science\\n3.1 Enzyme-Mediated\\nDDIs & Prediction\\nInvestigates cytochrome P450\\ninteractions and uses models to\\nforecast clinical risk.\\ncytochrome P450, PBPK modelling,\\nQT prolongation\\n14,5\\n3.2 Transporter-Mediated\\nDDIs & Biomarkers\\nStudies renal and hepatic\\ntransporters and endogenous probes\\nto detect interaction liability.\\nrenal transporters, hepatic\\ntransporters, endogenous biomarkers\\nof transporter activity\\n13,24,27\\n3.3 Clinical DDIs & Risk\\nManagement\\nDocuments real-world interaction\\nscenarios and strategies to mitigate\\nadverse outcomes.\\nopioid overdose management,\\nnirmatrelvir/ritonavir interactions,\\nEHR-based pharmacovigilance\\n0,8\\n4. Regulatory Science & Evidence Generation\\n4.1 Trial Diversity &\\nHealth Equity\\nPromotes representative enrolment\\nand equitable access in clinical\\nresearch.\\nclinical trial diversity, health equity,\\nregulatory initiatives\\n10\\n4.2 Real-World Evidence\\n& External Controls\\nLeverages observational data and\\nsynthetic controls to inform\\nregulatory decisions.\\nreal-world evidence, external control\\ntrials, regulatory frameworks\\n9\\n4.3 Drug-Lifecycle\\nOversight & Lag\\nAnalysis\\nEvaluates approval timelines,\\npost-marketing requirements, and\\nregulatory performance.\\nregulatory science, drug lag,\\npost-marketing studies\\n16\\n4.4 Biomarker / Rare\\nDisease / Biosimilar\\nQualification\\nEstablishes evidentiary standards for\\nbiomarkers, orphan products, and\\nbiosimilars.\\nrare-disease drug development,\\nbiomarker qualification, biosimilar\\ndevelopment, PK/PD biomarkers,\\nregulatory strategies\\n22,23\\n5. Clinical Therapeutics & Outcomes Research\\n5.1 Cardio-Renal &\\nMetabolic Outcomes\\nAssesses the long-term efficacy and\\nsafety of metabolic therapies on\\ncardiovascular and renal endpoints.\\nSGLT2 inhibitors,\\ncardiovascular–renal outcomes,\\nantidiabetic drug safety\\n12\\nTable 11 | Domain Survey for Science related Survey.\\n19'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 19}, page_content='Category\\nSub-category\\nWhat is covered\\nTypical examples\\nCluster\\n1. Textual inputs\\n1.1 Large-scale\\ntokenized corpora\\nMassive general-domain text for LM\\npre-training\\nWeb pages; Wikipedia; books; C4; Pile;\\nWikiText; OpenWebText; SlimPajama\\n11\\n1.2 Prompt &\\ninteraction data\\nUser/system prompts and model replies\\ngathered for alignment, RLHF or robustness\\nPrompts/questions; model responses;\\npreference/reward labels; adversarial triggers;\\nlong-context demonstrations\\n0, 2\\n1.3 Problem\\nstatements with\\ncontext\\nNatural-language tasks paired with explicit\\nstructured knowledge or code/data schemas\\nNL problem + knowledge graph/database\\nschema/code stub; reasoning traces or\\nstep-by-step solutions\\n14\\n2. Visual inputs (images)\\n2.1 Raw images\\nCanonical labelled/unlabelled images after\\nbasic augmentation\\nImageNet, CIFAR, COCO photos; medical\\nscans\\n19\\n2.2 Cued images\\nImages supplied with auxiliary\\nspatial/sensor cues\\nLow-light or blurry photos + masks; camera\\nposes; depth/event data; points/boxes\\n17\\n2.3 Patch or region\\ntokens\\nVisual patches embedded as tokens for\\ntransformer processing\\nViT/MAE patches from images or single\\nvideo frames\\n3\\n3. Video & motion inputs\\n3.1 Video streams\\nwith motion cues\\nTime-ordered frames plus motion/semantic\\nsignals\\nVideo frames; optical flow; 3-D pose;\\nsegmentation masks; aligned audio track\\n13\\n4. 3-D & spatial inputs\\n4.1 Geometry &\\ndepth representations\\nExplicit 3-D or depth data for spatial\\nreasoning\\nPoint clouds; RGB-D images; TSDF/voxel\\ngrids; meshes; camera extrinsics; semantic\\nlabels\\n1\\n5. Multimodal token sequences\\n5.1 Cross-modal\\ntoken bags\\nTokens from diverse modalities embedded\\nwith positional info\\nText, audio, vision, graphs, biology tokens\\nwith position vectors\\n12, 18\\n5.2 Encoder-fused\\ntokens\\nTokens from separate encoders\\nconcatenated into one sequence\\nCLIP/ViT image tokens + BERT/LLaMA text\\ntokens\\n15\\n5.3 Normalized latent\\nembeddings\\nModality-specific encoders map data into a\\nshared latent space (may include\\nplaceholders)\\nText, images, video, audio all →joint\\nembeddings (missing modalities allowed)\\n4\\n6. Generative-model conditioning\\n6.1 Diffusion noise\\nschedule\\nNoisy latent sample 𝑥𝑡, timestep token 𝑡,\\noptional class/text/geometry conditioning\\n𝑥𝑡+ 𝑧; timestep 𝑡; class label; pose map;\\ndepth; edges\\n16\\n6.2 Auxiliary\\ngeneration cues\\nUser-supplied hints steering image\\ngeneration or editing\\nReference image; mask; depth; pose; layout;\\nbounding boxes\\n10\\n7. Task-oriented multimodal inputs\\n7.1 Visual\\nobservations + NL\\nprompts\\nPerception frames paired with a\\nnatural-language task or edit instruction\\nScreenshot + “click the red button”; video\\nframe + “highlight the pedestrian”\\n6\\n7.2 Image-text pairs\\nwith cues\\nCaptioned/questioned images often carrying\\nregion annotations\\nImage + caption; VQA triplets; bounding-box\\n/ mask annotations\\n7\\n7.3 Embodied-agent\\ncontext\\nAgent perception, proprioception\\nhistory combined with a goal description &\\nRGB-D stream; past actions; goal text\\n(“navigate to the chair”)\\n8\\n8. Sequential & trajectory inputs\\n8.1 Offline\\nstate–action\\ntrajectories\\nLogged sequences for offline RL or\\nbehaviour cloning\\nTime-series control signals; graphs; 3-D\\nskeleton poses; human preference labels\\n9\\n9. Inverse-problem observations\\n9.1 Corrupted\\nmeasurements with\\nground truth\\nRaw measurements transformed by known\\noperators, paired with target outputs\\nMRI 𝑘-space + mask; blurred →sharp image\\npairs; noisy sensor data\\n5\\nTable 12 | Structured summary of input types used in foundation-model papers.\\n20'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 20}, page_content='Category\\nSub-category\\nWhat is covered\\nTypical examples\\nCluster\\n1. Representation & Architecture\\n1.1 Token & latent\\nrepresentation\\nMapping raw data to\\ndiscrete/continuous tokens or\\nlatents\\nLatent representation learning;\\ntoken/patch embedding; visual-token\\nprojection . . .\\n0, 14, 17, 18,\\n19\\n1.2 Attention &\\nTransformer variants\\nArchitectural changes that make\\nattention cheaper or deeper\\nSparse/low-rank attention; spatiotemporal\\nattention; positional scaling . . .\\n2, 10, 11, 14,\\n17, 18, 19\\n1.3 Mixture-of-Experts &\\nrouting\\nDynamic selection of expert\\nblocks or routes\\nModular MoE; dynamic routing; MoE\\ntoken routing . . .\\n0, 11, 15, 17,\\n19\\n2. Generative Paradigms\\n2.1 Diffusion &\\nscore-based generation\\nNoise-to-data generative flows\\nUNet diffusion; latent diffusion; guided\\nconditional sampling . . .\\n2, 3, 4, 5, 10,\\n13, 18\\n2.2 Energy-based &\\ncontrol formulations\\nSampling by minimising energy\\nor solving control processes\\nEnergy-based score learning;\\noptimal-control SDE; solver-accelerated\\ninversion . . .\\n13\\n2.3 Probabilistic &\\nmasked inference\\nNon-diffusion probabilistic\\ndecoders\\nProbabilistic generative inference;\\nmasked auto-encoding; next-token\\nprediction . . .\\n0, 17, 18, 19\\n3. Multimodal Alignment & Fusion\\n3.1 Encoders →shared\\nlatent space\\nSeparate encoders project each\\nmodality into a common space\\nModality-specific encoders; projection\\nlayers; frozen CLIP backbone . . .\\n1, 6, 12, 18,\\n19\\n3.2 Cross-attention fusion\\n& conditioning\\nMechanisms for interaction\\nbetween modalities\\nCross-attention fusion; prompt\\ncross-attention; multimodal concatenation\\n. . .\\n2, 4, 6, 10, 12,\\n14, 18, 19\\n3.3\\nVision/Video-language\\nalignment\\nAligning paired modalities in\\nlatent space\\nVideo-language alignment; contrastive\\nalignment; generative self-supervision . . .\\n1, 8, 12, 14,\\n18, 19\\n4. Adaptation & Efficiency\\n4.1 Parameter-efficient\\nadaptation\\nUpdating only a small subset of\\nweights or added modules\\nLoRA/adapters; low-rank tuning;\\nmodular fusion . . .\\n1, 2, 4, 10, 12,\\n15, 16, 19\\n4.2 Prompting & modular\\nextensions\\nSteering frozen backbones with\\nprompts or plug-ins\\nPrompt conditioning; chain-of-thought\\nprompting; tool invocation . . .\\n1, 6, 7, 9, 17,\\n19\\n4.3 Compression &\\nefficient training\\nReducing compute, memory or\\ntraining cost\\nQuantisation; pruning-distillation;\\ncommunication-efficient sharding . . .\\n2, 5, 10, 15,\\n16, 19\\n5. Reasoning & Interaction\\n5.1 Chain-of-thought &\\ntool reasoning\\nExplicit reasoning traces or calls\\nto external tools\\nChain-of-thought reasoning;\\nretrieval-augmented reasoning;\\nself-refinement . . .\\n6, 7, 8, 9\\n5.2 RL & preference\\nmodeling\\nReinforcement or\\npreference-based optimisation\\nPreference-conditioned policies; RLHF\\nalignment; optimal-transport RL . . .\\n3, 9\\n5.3 Multi-agent / planner\\nloops\\nMultiple interacting agents or\\nexplicit planner loops\\nMulti-agent collaboration;\\nPlanner-Actor-Corrector-Verifier loop . . .\\n7, 8\\n6. Robustness & Domain Shift\\n6.1 Uncertainty & robust\\noptimisation\\nEstimating confidence and\\nresisting adversarial inputs\\nUncertainty quantification; adaptive\\nmemory; adversarial robustness . . .\\n0, 9, 16\\n6.2 Domain adaptation &\\nmodel editing\\nAdapting or editing knowledge\\npost-training\\nTargeted model editing; synthetic-data\\nadaptation; knowledge probing . . .\\n4, 9, 16, 19\\nTable 13 | Structured summary of modeling techniques used in foundation-model papers.\\n21'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 21}, page_content='Category\\nSub-category\\nWhat is covered\\nTypical examples\\nCluster\\n1. Language-centric outputs\\n1.1 Token probabilities &\\nsequences\\nAutoregressive LMs: token\\nlogits or generated text\\nnext-token probability distributions; generated\\ntoken sequences\\n2\\n1.2 Aligned LLM responses\\nInstruction-tuned completions\\nfor reasoning, safety,\\nlong-context\\nhelpful-harmless-truthful responses; safe\\nrefusals; reasoning-enhanced;\\nhallucination-reduced\\n3, 15\\n1.3 Reasoning traces &\\nanswers\\nChain-of-thought steps plus\\nfinal decoded result\\nchain-of-thought reasoning; intermediate\\noutputs; final answers\\n0\\n1.4 Visually-grounded NL\\noutputs\\nLanguage grounded in\\nimage/video content\\ncaptions; visual-QA; reasoning with bounding\\nboxes, masks\\n9\\n2. Generative visual & multimodal outputs\\n2.1 Photorealistic images\\nHigh-fidelity images from text\\nor prompts\\nphotorealistic; identity-preserving;\\ncontext-coherent images; text-conditioned\\nimages\\n1, 18\\n2.2 Video / motion\\ngeneration\\nConsistent video or 3D motion\\nfrom text\\ntemporally-consistent video; 3-D motion\\ngeneration/editing\\n5\\n2.3 3D scenes & assets\\nMeshes, NeRFs, Gaussian fields\\nfor rendering/editing\\nmeshes; point clouds; NeRF; editable scene/asset\\ngeneration\\n11\\n2.4 Multimodal\\nreconstructions\\nImages, video, audio decoded\\nfrom latents\\nreconstructed/generated multi-modal data\\n19\\n2.5 Diffusion samples &\\nnoise\\nReverse-diffusion outputs with\\nnoise estimates\\ngenerated or reconstructed samples. . . plus\\nnoise/score estimates\\n7\\n3. Predictive & structured outputs\\n3.1 Classification scores\\nClass labels, probabilities, or\\nlogits\\nclass labels / probabilities / logits\\n4, 13\\n3.2 Localization &\\nsegmentation\\nMasks, boxes, poses, or\\ncaptions pinpointing content\\nsegmentation masks; bounding boxes; 3-D\\nlocalization/pose; textual grounding/captions\\n16, 13\\n3.3 Structured artefacts\\nGraphs, coordinates, flows,\\nmolecules, causal terms\\ngraphs; poses; flows; molecular/crystal\\nstructures; uncertainties; causal/physical\\nparameters\\n14\\n3.4 Downstream\\nembeddings\\nTransformed features for later\\ntask use\\ntransformed feature embeddings;\\nreconstructed/generative outputs\\n13\\n3.5 Control & planning\\nPredicted actions, plans, or\\ntrajectories\\naction sequences & control commands;\\ntask-grounded plans\\n6\\n4. Evaluation, improvement & efficiency outputs\\n4.1 Metrics & benchmarks\\nAccuracy, bias, uncertainty,\\nsafety, etc.\\naccuracy; F1; ROC-AUC; Elo; bias; calibration\\n17\\n4.2 Enhanced / corrected\\nartefacts\\nOutputs improving other models\\n(predictions, data, signals)\\ncorrected predictions; synthetic/augmented data;\\nanomaly/OOD scores; attribution indicators\\n10\\n4.3 Compressed models\\nQuantised/tuned checkpoints\\nwith reduced cost\\nefficient, compressed, fine-tuned foundation\\nmodels\\n8\\n5. Embedding-space outputs\\n5.1 Aligned multimodal\\nembeddings\\nJoint space for text/image/audio\\nenabling retrieval or\\nclassification\\naligned multimodal embeddings; zero-shot\\nclassification\\n12\\nTable 14 | Structured summary of output types used in foundation-model papers.\\n22'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 22}, page_content='Category\\nSub-category\\nWhat is covered\\nTypical examples\\nCluster\\n1. Language-modeling objectives\\nNext / Masked-token\\nPrediction\\nMinimize CE on next/masked token.\\nNext-/masked-token pred.; LM; CE/NLL\\nmin.; aux reg.\\n4\\nGeneral LLM\\nAdvancement\\nImprove reasoning, alignment,\\nefficiency, robustness.\\nReasoning; alignment; eval.; efficiency;\\nrobustness; multi-domain\\n12\\n2. Alignment & safety objectives\\nHuman-Preference\\nAlignment\\nMaximize learned reward; limit\\ndivergence.\\nPref. align; reward max.; safety-divergence\\nreg.\\n1\\nHallucination & Bias\\nMitigation\\nCut hallucinations/bias via\\ngrounding alignment.\\nHallucination det./mit.; x-modal\\nalign/ground; bias red.\\n0\\nGeneral Safety &\\nRobustness\\nLosses for safety, explainability,\\nrobust autonomy.\\nAlignment; safety; efficiency; general.;\\nexplain.; autonomy\\n6\\nSecurity & Privacy\\nDefense\\nDefend attacks, watermark, erase\\nconcepts.\\nAdv. robustness; watermark;\\nbackdoor/membership defense; privacy;\\nconcept erase; interp.\\n7\\n3. Adaptation & continual-learning\\nPrompt / Self-training\\nAdaptation\\nPrompt/pseudo-label adapt for\\nzero/few-shot.\\nFM adapt; prompt/pseudo-label;\\nzero-/few-shot OVR; robustness; domain\\ngen.\\n10\\nRetention-Regularized\\nFine-tuning\\nRegularize fine-tuning to retain\\nknowledge.\\nTask loss + retention reg.; preserve\\nknowledge; generalization\\n17\\n4. Multimodal objectives\\nUnified Multimodal\\nRepresentations\\nVision-language align, ground,\\nreason.\\nUnif. multimodal; V-L align; grounding;\\nx-modal reason.; zero/few-shot; cont.\\nadapt.\\n3\\nContrastive & Masked\\nAlignment\\nContrastive+masked for joint\\nembeddings.\\nX-modal contrast; masked recon.; joint\\nclass.; dist. align\\n13\\n3D / Multi-view\\nGeneration\\nCross-modal loss for 3D-consistent\\nviews.\\nHi-fid 3D multi-view gen.; sparse 2D/text\\n19\\n5. Generative diffusion objectives\\nCore Enhancement\\nFaster, higher-quality diffusion via\\nguidance.\\nAccelerate train/inf.; guide/loss opt.;\\nfidelity; diversity; control\\n16\\nNoise-prediction &\\nScore-matching\\nTrain via noise pred., reconstr.,\\nELBO.\\nNoise pred denoise; recon. fid. min.;\\nscore/ELBO opt.\\n18\\nVideo / Motion Diffusion\\nConditioned diffusion for coherent\\nvideo.\\nHi-fid coherent video/motion synth.;\\ncontrol; prompt align\\n2\\nControllable Image\\nDiffusion\\nSteer image diffusion for fairness\\netc.\\nAlign; personalise; fairness; diversity;\\nspatial; hi fid.; light train\\n5\\nLatent & Denoising\\nRegularization\\nExtra denoise/latent loss.\\nDenoise min.; latent align; cond. reg.; dist.\\nfid. train\\n8\\n6. Policy-learning & RL\\nMulti-task Policy RL\\nOne policy via cloning+pref. RL.\\nMulti-task policy; behavior/diffusion\\ncloning; pref.-aligned RL; reward exp.\\n14\\n7. Optimization & efficiency\\nLoss & Representation\\nMatching\\nMinimize task loss, align\\ndistributions.\\nTask combined losses; dist align; repr\\nmatch; reg. opt.\\n11, 15\\nCompute / Memory\\nEfficiency\\nCut compute/memory, keep\\naccuracy.\\nMin. compute/memory/param cost;\\ntrain/ft/inf.\\n9\\nTable 15 | Structured summary of learning objectives used in foundation-model papers.\\n23'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 23}, page_content='Category\\nSub-category\\nWhat is covered\\nTypical examples\\nCluster\\n1. Pre-Training & Representation Learning\\n1.1 Contrastive/masked\\nvision–language pre-training\\nLearn aligned image–text\\nembeddings before any\\ntask-specific tuning.\\nViT/CLIP contrastive/masked pretrain;\\nstrong aug.; 𝑇=0.07; 40–600 ep\\nfinetune.\\n5\\n1.2 Adapter-aided diffusion\\nimage/video pre-training\\nFreeze released checkpoints; add\\nlightweight adapters to scale.\\nFrozen ckpt + LoRA/prompt; AdamW\\n+ cos LR; prog-res; CF guidance.\\n13\\n2. Fine-Tuning & Adaptation\\n2.1 Vision–language instruction\\ntuning\\nTurn a frozen VLM into an\\ninstruction follower.\\nImage–text pretrain→inst. tune; PEFT;\\nopt. RLHF.\\n1\\n2.2 Parameter-efficient domain\\nadaptation\\nKeep backbone frozen; adapt via\\nprompts/adapters only.\\nPrompt/adapter/LoRA; distill or\\ncontrastive shift.\\n10\\n2.3 Instruction SFT + retrieval\\nalignment\\nAlign an LLM with retrieval and\\npreferences.\\nMulti-stage SFT; retrieval ctx;\\nDPO/RLHF; rerank→generate.\\n6\\n2.4 3-D coarse-to-fine diffusion\\nadaptation\\nMake diffusion/LLM backbones\\n3-D consistent.\\nAlt. SDS/guidance; synth views;\\nrender-denoise distill.\\n4\\n2.5 Video-diffusion adapter tuning\\nSpecialise image diffusion for\\ntemporal output.\\nTemp/spatial adapters; latent denoise;\\nlow→high-res.\\n7\\n2.6 Controllable diffusion\\nsampling\\nAdd style/identity knobs without\\nretraining core model.\\nVar-score-recon losses; dyn. guidance;\\nfeature mod.\\n8\\n2.7 Layout / prompt-conditioned\\ndiffusion\\nCondition generation on structured\\nlayouts or text.\\nLLM layout cond.; masked-attn\\nsampling; coarse→fine.\\n11\\n2.8 Composite-loss\\nself-supervised fine-tuning\\nImprove a backbone with multiple\\nunsupervised signals.\\nMask/noise; contrast+recon+distill;\\nEMA teacher.\\n15\\n2.9 Pseudo-label self-training\\nSelf-train using synthetic\\nmultimodal labels.\\nSynth labels (Diff/LLM/SAM); filter;\\nadapter FT; contrast/distill.\\n16\\n3. Reinforcement Learning & Control\\n3.1 Diffusion-backed policy\\noptimisation\\nBlend BC and RL signals for\\npolicy training.\\nTraj samp; BC+PPO; Q-guided\\ndenoise; self-play.\\n0\\n3.2 Hierarchical planning &\\nembodied control\\nCombine VLM/LLM skills with\\nrobotic policies.\\nSkill seg; hier plan; RH control;\\nreal-time accel.\\n19\\n4. Efficiency & Compression\\n4.1 Model compression &\\nquantisation\\nShrink models with minimal\\nretraining.\\nLow-rank+sparse; mixed-prec.;\\nprune+search.\\n2\\n4.2 Transformer training /\\ninference acceleration\\nArchitectural and parallel tricks to\\ncut runtime.\\nMulti-dev partition; sparse/flash attn;\\nKV prune; stride denoise.\\n9\\n4.3 Hyper-parameter &\\ninfrastructure optimisation\\nWell-tuned schedules and\\ndistributed stacks.\\nAdamW warm-cos LR; FP16/BF16;\\nDeepSpeed; 100k–500k steps.\\n17\\n5. Safety & Adversarial Robustness\\n7.1 Jailbreak & adversarial prompt\\nsynthesis\\nCraft inputs that bypass safety\\nguards.\\nHarmful data; shadow model; grad\\ntoken opt; synth prompt.\\n12\\nTable 16 | Structured summary of training recipes used in foundation-model papers.\\n24'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 24}, page_content='Category\\nSub-category\\nWhat is covered\\nTypical examples\\nCluster\\n1. Vision & Imaging Sensors\\n1.1 RGB cameras\\nMonocular, stereo, multi-view,\\nsurround-view or panoramic cameras\\nproducing color frames; used for\\nappearance-based perception.\\nfront/side/rear vehicle cameras,\\negocentric/wrist/-head cameras,\\naerial/on-board cameras\\n0, 1, 2, 3, 4, 6,\\n7, 9, 10, 11,\\n13, 14, 15, 16,\\n17, 18, 19\\n1.2 RGB-D cameras\\nActive or structured-light cameras that\\noutput synchronized color + depth images.\\nIntel RealSense, Azure Kinect,\\npanoramic RGB-D rigs\\n3, 7, 10, 11,\\n13, 14, 17, 18,\\n19\\n1.3 Event (neuromorphic)\\ncameras\\nAsynchronous sensors emitting per-pixel\\nbrightness changes with micro-second\\nlatency.\\nDVS, DAVIS\\n9\\n1.4 Thermal / LWIR\\ncameras\\nPassive long-wave IR imagers for\\ntemperature or night-vision cues.\\nThermal cameras, LWIR DoFP\\npolarization cameras\\n3, 14\\n2. Depth & Range Sensors\\n2.1 LiDAR\\nSpinning or solid-state laser scanners\\nreturning 3-D point-clouds.\\nMulti-beam/spinning LiDAR,\\nPolLidar wavefront lidar\\n3, 4, 5, 11, 13,\\n14, 16, 17\\n2.2 Time-of-Flight\\ncameras\\nPulsed or continuous-wave light cameras\\ncomputing per-pixel range.\\nIndirect/monocular ToF depth\\ncameras, AMCW-ToF\\n9, 14\\n2.3 Radar\\nmmWave / FMCW / 4-D imaging radars\\nmeasuring range–Doppler or heat-maps.\\nAutomotive mmWave/FMCW,\\nMIMO imaging radar\\n3, 4, 14\\n3. Proprioceptive Sensors\\n3.1 Joint & wheel\\nencoders\\nOptical or magnetic sensors giving joint\\nangle / wheel ticks.\\njoint encoders, wheel encoders\\n3, 7, 8, 13, 16\\n3.2 IMUs\\n3-axis accelerometers & gyros providing\\norientation/velocity.\\nIMU, pose modules\\n3, 4, 7, 13, 16,\\n17\\n3.3 Force / torque sensors\\nStrain-gauge or multi-axis transducers\\nmeasuring interaction forces.\\nforce–torque sensors,\\nmotor-current feedback\\n7, 13, 16, 19\\n3.4 Motor-current sensors\\nDrive-current read-back for inferred load.\\nmotor-current feedback\\n19\\n4. Tactile & Contact Sensors\\n4.1 Vision-based tactile\\nCamera-in-gel sensors capturing\\nhigh-resolution surface contact.\\nGelSight, Soft-Bubble\\n13\\n4.2 Pressure / tactile\\narrays\\nCapacitive or resistive skins giving\\nper-taxel pressure maps.\\nforce-torque/pressure arrays,\\ncontact sensors\\n7, 13\\n5. External Tracking & Global Localization\\n5.1 Optical\\nmotion-capture systems\\nInfra-red camera networks tracking\\nreflective markers.\\nVICON, optical marker rigs\\n3, 13, 14, 19\\n5.2 Wearable mocap\\ndevices\\nMarker gloves or body suits for fine\\nhuman-pose capture.\\nmotion-capture gloves,\\nskeletal/hand markers\\n13, 19\\n5.3 Radio-based\\npositioning\\nSatellite or UWB transceivers returning\\nglobal coordinates.\\nGPS, UWB beacons\\n3, 4, 16\\n6. Audio Sensors\\n6.1 Microphones / audio\\narrays\\nMono or array microphones for speech /\\nenvironmental sound.\\nmicrophone audio inputs\\n3, 13, 19\\nTable 17 | Structured summary of input sensors used in robotic papers.\\n25'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 25}, page_content='Category\\nSub-category\\nWhat is covered\\nTypical examples\\nCluster\\n1. Ground-based mobile robots\\n1.1 Small RC / off-road\\nvehicles\\n1/10-scale cars, ATVs,\\nskid-steer rovers for field tests\\nRC cars/ATVs; small off-road vehicles\\n19\\n1.2 Kinematic vehicle\\nmodels\\nBicycle/unicycle point-mass\\nmodels (simulation-only)\\nSimulated vehicle agents\\n(kinematic/dynamic)\\n0\\n2. Aerial robots\\n2.1 Quadrotors / drones\\nFour-rotor UAVs with cameras,\\nLiDAR, IMU\\nQuadrotor UAVs; drones\\n11, 19\\n3. Legged & humanoid robots\\n3.1 Simulated legged\\nagents\\nClassic MuJoCo bodies for RL\\nlocomotion\\nHopper; HalfCheetah; Walker2d; Ant;\\nQuadruped\\n11\\n3.2 Real quadrupeds &\\nhybrids\\nTorque-controlled ∼12-DoF\\nquadrupeds; wheel-leg hybrids\\nANYmal; Unitree A1/Go1; MIT\\nMini-Cheetah; wheel-leg hybrids\\n12\\n3.3 Humanoids\\nHigh-DoF bipeds/humanoids,\\noften with articulated hands\\nHumanoids/bipeds; SMPL-X mesh;\\nsimulated avatars\\n16\\n4. Manipulators & end-effectors\\n4.1 Standard 6–7 DoF\\narms\\nFixed-base arms with two-finger\\nor suction grippers\\nUR5e; Sawyer; other 6–7 DoF arms\\n2\\n4.2 Franka-class agile\\narms\\n7-DoF Panda-style arms popular\\nin RL/IL\\nFranka Emika Panda; Robotiq; suction\\ncups\\n3\\n4.3 Mobile / dual-arm\\nmanipulators\\nOne or two arms on a wheeled\\nbase (bimanual possible)\\nMobile bases with dual arms; mobile\\nmanipulators\\n7, 11\\n4.4 Arm + dexterous hand\\nArms distinguished by\\nmulti-finger hands\\nShadow; Allegro; Adroit; LEAP;\\nDeltaHand\\n16, 18\\n5. Soft & continuum robots\\n5.1 Continuum / soft\\nmanipulators\\nDeformable backbones,\\npneumatic/tendon actuation,\\nsoft skins & grippers; tensegrity\\nframes\\nSoft continuum arm; soft gripper;\\ncompliant tensegrity structures\\n1\\nTable 18 | Structured summary of physical bodies used in robotic papers.\\n26'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 26}, page_content='Category\\nSub-category\\nWhat is covered\\nTypical examples\\nCluster\\n1. Direct joint-level outputs\\n1.1 Joint state read-outs\\nInstantaneous articulated joint\\npositions, orientations, angles,\\nvelocities\\n“joint positions & orientations”; “joint\\nangles”; “joint velocities”; “mesh\\ndeformations”\\n0, 1, 9\\n1.2 Joint command\\nsignals\\nLow-level motor targets (torque\\n/ position / velocity) that drive\\njoint motion\\n“joint torque/position commands”;\\n“continuous motor control signals”; “PD\\ncontrol torques”\\n7, 11, 12, 16,\\n17\\n1.3 Joint motion\\ntrajectories\\nTime-indexed sequences of joint\\nstates the robot follows\\n“motion sequences over time”; “planned\\njoint trajectories”; “optimised 6-DoF\\ntrajectories”\\n0, 1, 17\\n2. Rigid-body / end-effector pose outputs\\n2.1 6-DoF body poses\\nPosition + orientation of whole\\nrobots, cameras or objects\\n“6-DoF poses”; “rigid transformations”;\\n“UAV 3-D position & orientation”\\n6, 10, 16\\n2.2 End-effector pose +\\ngripper\\nCartesian pose of manipulator\\ntip plus gripper open/close state\\n“6-DoF end-effector pose\\n(𝑥, 𝑦, 𝑧, 𝑟, 𝑝, 𝑦)”; “gripper_state\\n(open/close)”\\n4\\n3. Ground-vehicle / mobile-robot control outputs\\n3.1 Steering & pedal\\ncommands\\nLow-level automotive controls\\nfor heading and speed\\n“steering_angle”;\\n“acceleration/throttle”; “brake”\\n3\\n3.2 Wheel /\\ndifferential-drive\\nvelocities\\nBody-frame linear & angular\\nvelocity commands for\\nwheels/actuators\\n“linear & angular velocity motor\\ncommands”; “wheel/actuator motions”\\n14\\n3.3 Motion trajectories\\nPre-planned paths or waypoints\\nfor vehicle motion\\n“robot/vehicle motion trajectories”;\\n“position/orientation updates”\\n19, 16\\n4. Aerial-rotorcraft control outputs\\n4.1 Rotor thrust &\\nbody-rate commands\\nPer-rotor thrust/speed or\\nbody-rate inputs that place a\\nUAV in 3-D space\\n“rotor thrust/speed commands”;\\n“collective thrust & body-rate inputs”\\n6\\nTable 19 | Structured summary of joint outputs used in robotic papers.\\n27'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 27}, page_content='Category\\nSub-category\\nWhat is covered\\nTypical examples\\nCluster\\n1. Continuous Low-Level Actuation\\n1.1 Joint-space\\ncommands\\nDirect numerical inputs to\\nindividual joints or actuators,\\nbounded by hardware limits.\\njoint torques/positions/velocities;\\nhigh-dimensional joint commands;\\nbounded control inputs; finger-joint\\nconfigs; parametrised joint trajectories\\n0, 4, 6, 10, 12,\\n14, 18\\n1.2 Vehicle / body\\ndynamics commands\\nLow-level controls that change a\\nmobile base, ground-vehicle or\\naerial body state.\\nsteering angle; throttle / acceleration;\\nbraking; linear & angular velocity;\\nbody-rate thrust; speed/direction for\\nlocomotion; lane-keeping\\n0, 1, 7, 10, 12,\\n13, 15\\n2. Mid-Level Pose & Trajectory Control\\n2.1 End-effector &\\ngripper pose\\n6-DoF goals and\\ntime-parameterised trajectories\\nfor arms, grippers or aerial\\nmanipulators.\\ncontinuous 6-DoF poses; pose deltas (Δ𝑥,\\nΔ𝑦, Δ𝑧, Δroll, Δpitch, Δyaw); gripper\\nopen/close; gripper width/force; grasp\\ntrajectories\\n2, 6, 9, 10, 12,\\n14, 18\\n2.2 Base / waypoint\\ntrajectories\\nDesired paths, way-points or\\nvelocity profiles for the robot\\nbody or ego vehicle.\\nwaypoint/path-goal selection; future\\ntrajectory sequences; base linear &\\nangular velocity commands;\\nlane-change/merge trajectories\\n0, 1, 7, 10, 15,\\n19\\n3. High-Level Discrete Skills & Behaviour Primitives\\n3.1 Manipulation skills\\nObject-centred primitives that\\nparameterise targets, forces or\\nobject states.\\ngrasp/pick; place/drop; push/pull;\\nrotate/open/close; part deformation\\n0, 10, 18, 19\\n3.2 Locomotion &\\nnavigation skills\\nDiscrete moves or gait switches\\nfor repositioning the whole\\nrobot.\\nmove_forward/stop; turn_left/turn_right;\\ngait switch; lane keeping/change;\\novertaking/merging; “go to X”\\n0, 1, 10, 15,\\n19\\n3.3 Interaction &\\ninstruction skills\\nMultimodal actions expressed\\nthrough gesture, speech or scene\\nedits.\\ngesture actions; speech actions;\\ninstructional guidance; scene editing\\ncommands\\n0\\nTable 20 | Structured summary of action space used in robotic papers.\\n28'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 28}, page_content='Category\\nSub-category\\nWhat is covered\\nTypical examples\\nCluster\\n1. Autonomous-driving & Mobile-vehicle scenes\\n1.1 On-road urban /\\nsuburban / rural driving\\nReal or simulated road networks with\\ntraffic, road rules, and weather variation.\\nurban roads; highways;\\nintersections; traffic\\nlights/signs; . . .\\n1, 2, 6, 9, 12,\\n13, 19\\n1.2 Off-road,\\ncross-country & planetary\\nterrain\\nStructured or unstructured natural terrains\\nrequiring ground-robot locomotion.\\nuneven ground; sand; gravel;\\nsnow; . . .\\n11\\n2. Manipulation workspaces\\n2.1 Basic household\\ntabletop\\nSmall cluttered indoor bench for\\nreach-scale manipulation.\\ncluttered tabletop; household\\nobjects; articulated fixtures; .. .\\n0\\n2.2 Kitchen & household\\nbenchmark suites\\nStandardised kitchen/tabletop scenes from\\nRLBench, Meta-World, FrankaKitchen,\\nHabitat, Ravens, etc.\\nkitchen counters; RLBench\\nstation; FrankaKitchen; . . .\\n14, 17\\n2.3 Assembly & insertion\\ntables\\nContact-rich assembly surfaces with\\nprecisely shaped parts.\\nassembly workspace; peg-hole\\njoints; plug-socket joints; . . .\\n18\\n2.4 Shared lab / industrial\\nworkcells\\nPlanar or 3-D manipulation bays in labs or\\nfactories, often human-robot shared.\\nlab work surfaces; human-robot\\nzones; static & dynamic\\nobstacles; . . .\\n10\\n3. Embodied navigation & Scene-understanding worlds\\n3.1 Multi-room home /\\noffice interiors\\nPhotorealistic or simulated domestic &\\noffice floorplans for navigation and light\\nmanipulation.\\napartments; offices; corridors;\\ndynamic changes; . . .\\n7\\n3.2 Large-scale mixed\\nindoor-outdoor simulators\\nDynamic 3-D worlds with physics for\\npoint-goal, exploration, or\\nsocial-navigation tasks.\\nrooms; mazes; multiple agents;\\npartial observability; . . .\\n15\\n3.3 Object-rich\\nmixed-reality scene sets\\nReal + synthetic household, lab, or\\nindustrial spaces emphasising clutter &\\ndiversity.\\nhousehold rooms; industrial\\nfloors; cluttered indoor scenes;\\n. . .\\n4, 5, 8\\n4. Physics-centric control benchmarks\\n4.1 Classic locomotion &\\nmanipulation suites\\nWidely-used control benchmarks with\\ndomain-randomised dynamics.\\nMuJoCo tasks; IsaacGym\\nwalkers; Robotarium arena; . . .\\n3\\n4.2 High-fidelity\\nmulti-physics platforms\\nEnvironments that model contact, fluids,\\ndeformables & human interaction in\\nindoor/outdoor scenes.\\nrigid-body scenes; deformable\\nobjects; fluid interaction;\\nhumans; . . .\\n16\\nTable 21 | Structured summary of environment used in robotic papers.\\n29'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 29}, page_content='1. Vision-Language Pretraining, Zero-Shot Learning,\\nCross-Modal Retrieval\\n2. Neural Radiance Fields, 3D Gaussian Splatting,\\nDynamic Scene Reconstruction\\n3. Neural Style Transfer, Diffusion-Based Generation,\\nControllable Editing\\n4. GAN, Image Inpainting/Translation, Disentangled\\nRepresentation\\n5. Embodied Navigation, 3D Vision-Language\\nGrounding, Scene Graph Generation\\n6. Adversarial Robustness, Federated Learning, Deepfake\\nDetection\\n7. 3D Scene Generation, Neural Radiance Fields,\\nDiffusion Models\\n8. Multimodal Large Language Models, Visual\\nGrounding & Reasoning, Benchmark Datasets\\n9. 3D Human Pose Estimation, Human-Object\\nInteraction, Transformer Motion Generation\\n10. Trajectory Prediction, HD-Map/Lane Generation,\\nData-Driven Traffic Simulation\\n11. 3D Human Avatars, Neural Rendering, Pose-Driven\\nAnimation\\n12. Anomaly Detection, Self-Supervised Learning,\\nMultimodal Vision\\n13. Event-Based Vision, Computational Imaging,\\nDepth/HDR Reconstruction\\n14. Audio-Visual Learning, Sign Language Processing,\\nGaze Estimation\\n15. Image/Video Matting, Trimap/Mask Guidance,\\nTransformer-Based Models\\n16. Semi-Supervised Segmentation, Pseudo-Label\\nConsistency, Medical Image\\n17. 6D Pose Estimation, Visual Localization, Equivariant\\nFeatures\\n18. Depth Estimation, Stereo Matching, 3D\\nReconstruction\\n19. Object Tracking, Transformer Models, UAV\\nSurveillance\\n20. LiDAR Point Clouds, 3D Object Detection, Semantic\\nSegmentation\\n21. Temporal Action Localization, Video Representation\\nLearning, Multimodal Reasoning\\n22. Image/Video Restoration, Neural Compression,\\nDiffusion Models\\n23. Few-Shot Learning, Continual Learning, Object\\nDetection\\n24. Domain Adaptation, Domain Generalization,\\nTest-Time Adaptation\\n25. Correspondence, Registration, Optical Flow\\n26. Person Re-Identification, Domain Generalization,\\nLarge-Scale Datasets\\n27. Adversarial Robustness, Long-Tailed Recognition,\\nOut-of-Distribution Detection\\n28. Representation Learning, Knowledge Distillation,\\nExplainability\\n29. Efficient ViT, Neural Architecture Compression,\\nSparse/Quantized NAS\\n30. Object Detection, Semantic Segmentation,\\nSelf-Supervised Learning\\nFigure 5 | Trend Visualization of Computer Vision Research\\n30'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 30}, page_content='1. Legged Locomotion, Reinforcement Learning,\\nSim-to-Real Transfer\\n2. Teleoperation, Dexterous Manipulation, Low-Cost\\nOpen-Source Robotics\\n3. Language-Conditioned Manipulation,\\nVision-Language-Action Models, 3D Scene Grounding\\n4. LLM-Robotics Integration, Open-Vocabulary Scene\\nMapping, Language-Driven Task Planning\\n5. Large-Scale Robotic Datasets, Simulation\\nEnvironments, Cross-Embodiment Learning\\n6. Diffusion Policies, Equivariant Learning, Robotic\\nManipulation\\n7. State Estimation, Learning-Based SLAM, Implicit 3D\\nMapping\\n8. Video Imitation Learning, Dexterous Bimanual\\nManipulation, Object-Centric Affordances\\n9. Dexterous Manipulation, Learning-Based Control,\\nSim-to-Real Transfer\\n10. Interactive Robot Learning, Assistive Feeding, User\\nPreference Adaptation\\n11. Learning-Based Terrain Traversability, Proprioceptive\\nState Estimation, Off-Road Robot Navigation\\n12. Mapless Navigation, Semantic-Topological Memory,\\nDeep Learning Exploration\\n13. Tactile Sensing, Multimodal Perception, Contact-Rich\\nManipulation\\n14. Deep Reinforcement Learning, Sim-to-Real Transfer,\\nRobust Robot Control\\n15. Quadrotor Control, Learning-Based Model Predictive\\nControl, Agile Flight\\n16. Motion Planning, Manifold Geometry,\\nLearning-Based Optimization\\n17. TAMP, Skill Hierarchy, Long-Horizon Manipulation\\n18. 6-DoF Grasping, Equivariant Neural Representations,\\nSim-to-Real Transfer\\n19. Imitation Learning, Robotic Manipulation, Data\\nEfficiency\\n20. Deformable Object Manipulation, Learning-Based\\nDynamics Modeling, Particle-Based 3D Representations\\n21. Human-Robot Collaboration, Interactive Learning,\\nProactive Assistance\\n22. Multi-Robot Exploration, Uncertainty Modeling,\\nRisk-Aware Planning\\n23. Safe Reinforcement Learning, Control Barrier\\nFunctions, Reachability Analysis\\n24. Robot Co-Design, Differentiable Simulation, Soft\\nRobotics\\n25. Pose Estimation, SLAM Mapping, Point Cloud\\nRegistration\\n26. 3D Object Detection, Multi-Sensor Fusion,\\nAutonomous Driving\\n27. Vision-Based Manipulation, Object-Centric Scene\\nRepresentations, Affordance-Driven Rearrangement\\n28. Offline Reinforcement Learning, Robotic Skill\\nLearning, Continual Adaptation\\n29. Trajectory Prediction, Safety-Critical Scenario\\nGeneration, Autonomous Driving Simulation\\n30. Robotic Reinforcement Learning, Skill-Based\\nManipulation, Sample-Efficient Learning\\nFigure 6 | Trend Visualization of Robotics Research\\n31'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 31}, page_content='1. LLM Evaluation, Alignment Methods, Human\\nFeedback\\n2. LLM Agents, Interactive Planning, Theory of Mind\\n3. Large Language Models, Mathematical Reasoning,\\nChain-of-Thought Prompting\\n4. Adversarial Robustness, Backdoor Attacks, Privacy\\nPreservation\\n5. Hallucination, Knowledge Editing, Calibration\\n6. Dense Retrieval, Open-Domain Question Answering,\\nRetrieval-Augmented Generation\\n7. Continual Learning, Instruction Tuning, In-Context\\nLearning\\n8. Vision-Language, Multimodal Pretraining,\\nCross-Modal Reasoning\\n9. Parameter-Efficient Fine-Tuning, Model Compression,\\nLarge Language Models\\n10. Misinformation Detection, AI-Generated Text\\nDetection, LLM Value Alignment\\n11. Code Generation, Large Language Models,\\nBenchmark Evaluation\\n12. Speech Translation, Multimodal Learning,\\nLow-Resource Speech\\n13. Low-Resource Languages, Multilingual Language\\nModels, Cross-Lingual Transfer\\n14. Transformer Efficiency, Long-Context Modeling,\\nAdaptive Computation\\n15. Social Bias, Debiasing, Fairness Evaluation\\n16. Style Transfer, Controllable Text Generation,\\nDisentangled Representations\\n17. Hate Speech Detection, Empathetic Dialogue,\\nMultimodal Analysis\\n18. N/A\\n19. Fact Verification, Misinformation Detection, Evidence\\nRetrieval\\n20. Knowledge Graph Embedding, Event Causality\\nReasoning, Temporal Knowledge Reasoning\\n21. Interpretability, Counterfactual Augmentation,\\nFew-Shot Prompt Tuning\\n22. Evaluation Metrics, Data Augmentation, Figurative\\nLanguage\\n23. Sentiment Analysis, Emotion Recognition, Argument\\nMining\\n24. Text-to-SQL, Table Question Answering,\\nData-to-Text Generation\\n25. Summarization, Evaluation, Keyphrase\\n26. Neural Machine Translation, Knowledge Distillation,\\nMultilingual Modeling\\n27. Syntactic Parsing, Compositional Generalization,\\nLanguage Model Probing\\n28. Dialogue Systems, Response Generation, Dialogue\\nState Tracking\\n29. Multilingual Representation Learning, Sentence\\nEmbeddings, Contrastive Learning\\n30. Named Entity Recognition, Relation Extraction,\\nEvent Extraction\\nFigure 7 | Trend Visualization of NLP Research\\n32'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 32}, page_content='1. Program Synthesis, Code Generation, Theorem Proving\\n2. Vision-Language Reasoning, Knowledge Graph\\nLearning, Compositional Generalization\\n3. Language Modeling, Retrieval Augmentation,\\nRepresentation Learning\\n4. Generative Modeling, Image Synthesis & Editing,\\nDiffusion-Based Methods\\n5. 3D Shape Generation, Neural Implicit Representations,\\nPoint-Cloud Reconstruction\\n6. Dialogue Systems, Multi-Agent Collaboration,\\nReinforcement Learning\\n7. Adversarial Robustness, Machine Unlearning,\\nDifferential Privacy\\n8. Efficient Transformer Architectures,\\nParameter-Efficient Fine-Tuning, Multilingual Adaptation\\n9. Video Understanding, Temporal Modeling, 3D\\nPerception\\n10. Equivariant GNNs, 3D Molecular Generation, Drug\\nDiscovery\\n11. Sparse Network Pruning, Embedding Compression,\\nRecommendation Systems\\n12. Vision Transformers, Object Detection,\\nSelf-Supervised Learning\\n13. Combinatorial Optimization, Causal Inference,\\nBayesian Optimization\\n14. Embodied AI, Robotic Manipulation, Differentiable\\nSimulation\\n15. Multi-Agent RL, Bandit Algorithms, Game-Theoretic\\nLearning\\n16. Recurrent Neural Networks, Neuroscience-Inspired\\nModeling, Theoretical Analysis\\n17. Text-to-Speech, Audio-Visual, Diffusion\\n18. Generative Modeling, Optimal Transport, Diffusion\\nModels\\n19. Neural Differential Equations, Physics-Informed\\nOperator Learning, Spatiotemporal Forecasting\\n20. Non-Convex Optimization, Stochastic Gradient\\nMethods, Convergence Analysis\\n21. Federated Learning, Differential Privacy, Robust\\nOptimization\\n22. Graph Neural Networks, Expressivity, Robustness\\n23. Equivariant Neural Networks, Group Symmetry,\\nGeometric Deep Learning\\n24. Uncertainty Estimation, Conformal Prediction, Model\\nInterpretability\\n25. Contrastive Learning, Disentangled Representations,\\nClustering\\n26. Adversarial Robustness, Backdoor Attacks, Model\\nSecurity\\n27. Continual Learning, Few-Shot Learning, Domain\\nAdaptation\\n28. Robustness, Knowledge Distillation, Distribution\\nShift\\n29. Reinforcement Learning, Offline Learning, Sample\\nEfficiency\\n30. Network Pruning, Low-Precision Quantization,\\nEfficient Architecture Search\\nFigure 8 | Trend Visualization of Machine Learning Research\\n33'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 33}, page_content='References\\n[1] S. Addepalli, K. Bhogale, P. Dey, and R. V. Babu. Towards efficient and effective self-supervised\\nlearning of visual representations. In European Conference on Computer Vision, 2024.\\n[2] C. Agia, R. Sinha, J. Yang, Z. Cao, R. Antonova, M. Pavone, and J. Bohg. Unpacking failure\\nmodes of generative policies: Runtime monitoring of consistency and progress. In Conference on\\nRobot Learning, 2024.\\n[3] A. Ajith, M. Xia, A. Chevalier, T. Goyal, D. Chen, and T. Gao. Litsearch: A retrieval benchmark\\nfor scientific literature search. arXiv preprint arXiv:2407.18940, 2024.\\n[4] A. Aubret, C. Teulière, and J. Triesch. Self-supervised visual learning from interactions with\\nobjects. In European Conference on Computer Vision, 2024.\\n[5] M. Augustin, A. Meinke, and M. Hein. Adversarial robustness on in- and out-distribution improves\\nexplainability. In European Conference on Computer Vision, 2024.\\n[6] J. Baek, S. K. Jauhar, S. Cucerzan, and S. J. Hwang. Researchagent: Iterative research idea\\ngeneration over scientific literature with large language models. arXiv preprint arXiv:2404.07738,\\n2024.\\n[7] G. Balakrishnan, Y. Xiong, W. Xia, and P. Perona. Towards causal benchmarking of bias in face\\nanalysis algorithms. In European Conference on Computer Vision, 2024.\\n[8] H. Bansal, A. Hosseini, R. Agarwal, V. Q. Tran, and M. Kazemi. Smaller, weaker, yet better:\\nTraining llm reasoners via compute-optimal sampling. In International Conference on Learning\\nRepresentations, 2025.\\n[9] L. Barcellona, A. Zadaianchuk, D. Allegro, S. Papa, S. Ghidoni, and E. Gavves. Dream to\\nmanipulate: Compositional world models empowering robot imitation learning with imagination.\\nIn International Conference on Learning Representations, 2025.\\n[10] S. Belkhale, T. Ding, T. Xiao, P. Sermanet, Q. Vuong, J. Tompson, Y. Chebotar, D. Dwibedi, and\\nD. Sadigh. Rt-h: Action hierarchies using language. In Robotics: Science and Systems, 2024.\\n[11] K. Bhardwaj, N. P. Pandey, S. Priyadarshi, V. Ganapathy, S. Kadambi, R. Esteves, S. Borse,\\nP. Whatmough, R. Garrepalli, M. V. Baalen, H. Teague, and M. Nagel. Sparse high rank adapters.\\nIn Neural Information Processing Systems, 2024.\\n[12] R. Bommasani.\\nOn the opportunities and risks of foundation models.\\narXiv preprint\\narXiv:2108.07258, 2021.\\n[13] S. Brahmbhatt, C. Tang, C. D. Twigg, C. C. Kemp, and J. Hays. Contactpose: A dataset of grasps\\nwith object contact and hand pose. In European Conference on Computer Vision, 2024.\\n[14] T. Broedermann, D. Brüggemann, C. Sakaridis, K. Ta, O. Liagouris, J. Corkill, and L. V. Gool.\\nMuses: The multi-sensor semantic perception dataset for driving under uncertainty. In European\\nConference on Computer Vision, 2024.\\n[15] A. Brown, C.-Y. Fu, O. Parkhi, T. L. Berg, and A. Vedaldi. End-to-end visual editing with a\\ngeneratively pre-trained artist. In European Conference on Computer Vision, 2024.\\n[16] U. Buchler, B. Brattoli, and B. Ommer. Improving spatiotemporal self-supervision by deep\\nreinforcement learning. In European Conference on Computer Vision, 2024.\\n34'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 34}, page_content='[17] J. Cao, Z. Gan, Y. Cheng, L. Yu, Y.-C. Chen, and J. Liu. Behind the scene: Revealing the secrets of\\npre-trained vision-and-language models. In European Conference on Computer Vision, 2024.\\n[18] S. Cen, J. Mei, K. Goshvadi, H. Dai, T. Yang, S. Yang, D. Schuurmans, Y. Chi, and B. Dai. Value-\\nincentivized preference optimization: A unified approach to online and offline rlhf. In International\\nConference on Learning Representations, 2025.\\n[19] L. Chambon, E. Zablocki, M. Chen, F. Bartoccioni, P. Pérez, and M. Cord. Pointbev: A sparse\\napproach for bev predictions. In Conference on Computer Vision and Pattern Recognition, 2024.\\n[20] Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi, C. Wang, Y. Wang, et al.\\nA survey on evaluation of large language models. ACM transactions on intelligent systems and\\ntechnology, 15(3):1–45, 2024.\\n[21] P. Chattopadhyay, Y. Balaji, and J. Hoffman. Learning to balance specificity and invariance for in\\nand out of domain generalization. In European Conference on Computer Vision, 2024.\\n[22] B. Chen, D. M. Monsó, Y. Du, M. Simchowitz, R. Tedrake, and V. Sitzmann. Diffusion forcing:\\nNext-token prediction meets full-sequence diffusion. In Neural Information Processing Systems,\\n2024.\\n[23] C. Chen, F. Tung, N. Vedula, and G. Mori. Constraint-aware deep neural network compression. In\\nEuropean Conference on Computer Vision, 2024.\\n[24] D. Z. Chen, A. X. Chang, and M. Nießner. Scanrefer: 3d object localization in rgb-d scans using\\nnatural language. In European Conference on Computer Vision, 2024.\\n[25] H. Chen*, S. Xie, S.-N. Lim, and A. Shrivastava. Fast encoding and decoding for implicit video\\nrepresentation. In European Conference on Computer Vision, 2024.\\n[26] M. Chen, Y. Li, Y. Yang, S. Yu, B. Lin, and X. He. Automanual: Constructing instruction manuals\\nby llm agents via interactive environmental learning. In Neural Information Processing Systems,\\n2024.\\n[27] Q. Chen, A. Walsman, M. Memmel, K. Mo, A. Fang, D. Fox, and A. Gupta. Urdformer: A pipeline\\nfor constructing articulated simulation environments from real-world images. In Robotics: Science\\nand Systems, 2024.\\n[28] S. Chen, P.-L. Guhur, M. Tapaswi, C. Schmid, and I. Laptev.\\nLearning from unlabeled 3d\\nenvironments for vision-and-language navigation. In European Conference on Computer Vision,\\n2024.\\n[29] X. Chen and J. Yang. X-fi: A modality-invariant foundation model for multimodal human sensing.\\nIn International Conference on Learning Representations, 2025.\\n[30] X. Chen, J. Djolonga, P. Padlewski, B. Mustafa, S. Changpinyo, J. Wu, C. R. Ruiz, S. Goodman,\\nX. Wang, Y. Tay, S. Shakeri, M. Dehghani, D. Salz, M. Lucic, M. Tschannen, A. Nagrani, H. Hu,\\nM. Joshi, B. Pang, C. Montgomery, P. Pietrzyk, M. Ritter, A. Piergiovanni, M. Minderer, F. Pavetic,\\nA. Waters, G. Li, I. Alabdulmohsin, L. Beyer, J. Amelot, K. Lee, A. P. Steiner, Y. Li, D. Keysers,\\nA. Arnab, Y. Xu, K. Rong, A. Kolesnikov, M. Seyedhosseini, A. Angelova, X. Zhai, N. Houlsby,\\nand R. Soricut. On scaling up a multilingual vision and language model. In Conference on\\nComputer Vision and Pattern Recognition, 2024.\\n35'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 35}, page_content='[31] Y. Chen, T. Wang, T. Wu, X. Pan, K. Jia*, and Z. Liu. Comboverse: Compositional 3d assets\\ncreation using spatially-aware diffusion guidance. In European Conference on Computer Vision,\\n2024.\\n[32] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu, B. Li,\\nP. Luo, T. Lu, Y. Qiao, and J. Dai. Internvl: Scaling up vision foundation models and aligning for\\ngeneric visual-linguistic tasks. In Conference on Computer Vision and Pattern Recognition, 2024.\\n[33] Z. Chen, D. Chen, R. Sun, W. Liu, and C. Gan. Scaling autonomous agents via automatic reward\\nmodeling and planning. In International Conference on Learning Representations, 2025.\\n[34] X. Cheng, Y. Ji, J. Chen, R. Yang, G. Yang, and X. Wang. Expressive whole-body control for\\nhumanoid robots. In Robotics: Science and Systems, 2024.\\n[35] X. Cheng, J. Li, S. Yang, G. Yang, and X. Wang. Open-television: Teleoperation with immersive\\nactive visual feedback. In Conference on Robot Learning, 2024.\\n[36] C. Chi, Z. Xu, C. Pan, E. Cousineau, B. Burchfiel, S. Feng, R. Tedrake, and S. Song. Universal\\nmanipulation interface: In-the-wild robot teaching without in-the-wild robots. In Robotics: Science\\nand Systems, 2024.\\n[37] H. Cho, M. Kato, Y. Sakai, and N. Inoue. Revisiting in-context learning inference circuit in large\\nlanguage models. In International Conference on Learning Representations, 2025.\\n[38] S. Choi, S. Yang, S. Choi, and S. Yun. Improving test-time adaptation via shift-agnostic weight\\nregularization and nearest source prototypes. In European Conference on Computer Vision, 2024.\\n[39] W. Choi, J. Park, S. Ahn, D. Lee, and H. Woo. Nesyc: A neuro-symbolic continual learner for com-\\nplex embodied tasks in open domains. In International Conference on Learning Representations,\\n2025.\\n[40] Y. Chow, G. Tennenholtz, I. Gur, V. Zhuang, B. Dai, A. Kumar, R. Agarwal, S. Thiagarajan,\\nC. Boutilier, and A. Faust. Inference-aware fine-tuning for best-of-n sampling in large language\\nmodels. In International Conference on Learning Representations, 2025.\\n[41] X. Chu, J. Su, B. Zhang, and C. Shen. Visionllama: A unified llama backbone for vision tasks. In\\nEuropean Conference on Computer Vision, 2024.\\n[42] A. Curtis, N. Kumar, J. Cao, T. Lozano-Pérez, and L. P. Kaelbling. Trust the proc3s: Solving\\nlong-horizon robotics problems with llms and constraint satisfaction. In Conference on Robot\\nLearning, 2024.\\n[43] A. Curtis, G. Matheos, N. Gothoskar, V. Mansinghka, J. B. Tenenbaum, T. Lozano-Pérez, and L. P.\\nKaelbling. Partially observable task and motion planning with uncertainty and risk awareness. In\\nRobotics: Science and Systems, 2024.\\n[44] P. Ding, H. Zhao, W. Zhang, W. Song, M. Zhang, S. Huang, N. Yang, and D. Wang. Quar-vla:\\nVision-language-action model for quadruped robots. In European Conference on Computer Vision,\\n2024.\\n[45] L. Doorenbos, R. Sznitman, and P. Márquez-Neila. Data invariants to understand unsupervised\\nout-of-distribution detection. In European Conference on Computer Vision, 2024.\\n[46] R. Doshi, H. R. Walke, O. Mees, S. Dasari, and S. Levine. Scaling cross-embodied learning: One\\npolicy for manipulation, navigation, locomotion and aviation. In Conference on Robot Learning,\\n2024.\\n36'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 36}, page_content='[47] J. Duan, W. Yuan, W. Pumacay, Y. R. Wang, K. Ehsani, D. Fox, and R. Krishna. Manipulate-\\nanything: Automating real-world robots using vision-language models. In Conference on Robot\\nLearning, 2024.\\n[48] A. Eldesokey and P. Wonka. Build-a-scene: Interactive 3d layout control for diffusion-based image\\ngeneration. In International Conference on Learning Representations, 2025.\\n[49] E. Elhamifar and D. Huynh. Self-supervised multi-task procedure learning from instructional\\nvideos. In European Conference on Computer Vision, 2024.\\n[50] J. Fang, A. Shafiee, H. Abdel-Aziz, D. Thorsley, G. Georgiadis, and J. H. Hassoun. Post-training\\npiecewise linear quantization for deep neural networks. In European Conference on Computer\\nVision, 2024.\\n[51] R. Feng, J. Hu, W. Xia, T. Gao, A. Shen, Y. Sun, B. Fang, and D. Hu. Anytouch: Learning unified\\nstatic-dynamic representation across multiple visuo-tactile sensors. In International Conference on\\nLearning Representations, 2025.\\n[52] D. J. Foster, A. Block, and D. Misra. Is behavior cloning all you need? understanding horizon in\\nimitation learning. In Neural Information Processing Systems, 2024.\\n[53] Y. Fu, S. Liu, A. Kulkarni, J. Kautz, A. A. Efros, and X. Wang. Colmap-free 3d gaussian splatting.\\nIn Conference on Computer Vision and Pattern Recognition, 2024.\\n[54] Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn. Humanplus: Humanoid shadowing and imitation\\nfrom humans. In Conference on Robot Learning, 2024.\\n[55] I. O. Gallegos, R. A. Rossi, J. Barrow, M. M. Tanjim, S. Kim, F. Dernoncourt, T. Yu, R. Zhang, and\\nN. Ahmed. Bias and fairness in large language models: A survey. Computational Linguistics, 50:\\n1097–1179, 2023.\\n[56] X. Gao, S. Dong, Y. He, Q. Wang, and Y. Gong. Beyond prompt learning: Continual adapter for\\nefficient rehearsal-free continual learning. In European Conference on Computer Vision, 2024.\\n[57] H. Geng, S. Wei, C. Deng, B. Shen, H. Wang, and L. Guibas. Sage: Bridging semantic and\\nactionable parts for generalizable articulated-object manipulation under language instructions. In\\nRobotics: Science and Systems, 2024.\\n[58] D. Ghosh, H. R. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna, T. Kreiman, C. Xu,\\nJ. Luo, Y. L. Tan, L. Y. Chen, Q. Vuong, T. Xiao, P. R. Sanketi, D. Sadigh, C. Finn, and S. Levine.\\nOcto: An open-source generalist robot policy. In Robotics: Science and Systems, 2024.\\n[59] A. Gomez-Villa, D. Goswami, K. Wang, A. Bagdanov, B. Twardowski, and J. van de Weijer.\\nExemplar-free continual representation learning via learnable drift compensation. In European\\nConference on Computer Vision, 2024.\\n[60] A. Goyal, V. Blukis, J. Xu, Y. Guo, Y.-W. Chao, and D. Fox. Rvt-2: Learning precise manipulation\\nfrom few demonstrations. In Robotics: Science and Systems, 2024.\\n[61] R. Grandia, E. Knoop, M. A. Hopkins, G. Wiedebach, J. Bishop, S. Pickles, D. Müller, and\\nM. Bächer. Design and control of a bipedal robotic character. In Robotics: Science and Systems,\\n2024.\\n[62] X. Gu and M. Krenn. Forecasting high-impact research topics via machine learning on evolving\\nknowledge graphs. Machine Learning: Science and Technology, 6(2):025041, 2025.\\n37'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 37}, page_content='[63] X. Gu, Y. Guo, Z. Li, J. Qiu, Q. Dou, Y. Liu, B. Lo, and G.-Z. Yang. Tackling long-tailed category\\ndistribution under domain shifts. In European Conference on Computer Vision, 2024.\\n[64] X. Gu, Y.-J. Wang, X. Zhu, C. Shi, Y. Guo, Y. Liu, and J. Chen. Advancing humanoid locomotion:\\nMastering challenging terrains with denoising world model learning. In Robotics: Science and\\nSystems, 2024.\\n[65] Y. Gui, Y. Jin, and Z. Ren. Conformal alignment: Knowing when to trust foundation models with\\nguarantees. In Neural Information Processing Systems, 2024.\\n[66] Z. Guo and T. Jin. Smoothing the shift: Towards stable test-time adaptation under complex\\nmultimodal noises. In International Conference on Learning Representations, 2025.\\n[67] J. Günster, P. Liu, J. Peters, and D. Tateo. Handling long-term safety and uncertainty in safe\\nreinforcement learning. In Conference on Robot Learning, 2024.\\n[68] G. Han*, J. Hur, J. Choi, and J. Kim*. Learning neural deformation representation for 4d dynamic\\nshape generation. In European Conference on Computer Vision, 2024.\\n[69] M. Han, Y. Zhu, S.-C. Zhu, Y. N. Wu, and Y. Zhu. Interpret: Interactive predicate learning from\\nlanguage feedback for generalizable task planning. In Robotics: Science and Systems, 2024.\\n[70] N. Hansen, J. S. V, V. Sobal, Y. LeCun, X. Wang, and H. Su. Hierarchical world models as visual\\nwhole-body humanoid controllers. In International Conference on Learning Representations, 2025.\\n[71] D. Harwath, A. Recasens, D. Suris, G. Chuang, A. Torralba, and J. Glass. Jointly discovering\\nvisual objects and spoken words from raw sensory input. In European Conference on Computer\\nVision, 2024.\\n[72] S. Hayou, N. Ghosh, and B. Yu. The impact of initialization on lora finetuning dynamics. In Neural\\nInformation Processing Systems, 2024.\\n[73] H. He, J. B. Li, X. Jiang, and H. Miller. Smt: Fine-tuning large language models with sparse\\nmatrices. In International Conference on Learning Representations, 2025.\\n[74] T. He, Z. Luo, X. He, W. Xiao, C. Zhang, W. Zhang, K. M. Kitani, C. Liu, and G. Shi. Omnih2o:\\nUniversal and dexterous human-to-humanoid whole-body teleoperation and learning. In Conference\\non Robot Learning, 2024.\\n[75] T. He, C. Zhang, W. Xiao, G. He, C. Liu, and G. Shi. Agile but safe: Learning collision-free\\nhigh-speed legged locomotion. In Robotics: Science and Systems, 2024.\\n[76] Y. He, G. Huang, P. Feng, Y. Lin, Y. Zhang, H. Li, et al. Pasa: An llm agent for comprehensive\\nacademic paper search. arXiv preprint arXiv:2501.10120, 2025.\\n[77] S. Hecker, D. Dai, and L. V. Gool. End-to-end learning of driving models with surround-view\\ncameras and route planners. In European Conference on Computer Vision, 2024.\\n[78] H. Hu, S. Mirchandani, and D. Sadigh. Imitation bootstrapped reinforcement learning. In Robotics:\\nScience and Systems, 2024.\\n[79] J. Y.-C. Hu, W.-P. Wang, A. Gilani, C. Li, Z. Song, and H. Liu. Fundamental limits of prompt\\ntuning transformers: Universality, capacity and efficiency. In International Conference on Learning\\nRepresentations, 2025.\\n38'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 38}, page_content='[80] S. Hu, C. Lu, and J. Clune. Automated design of agentic systems. In International Conference on\\nLearning Representations, 2025.\\n[81] Y. Hu, S. Chai, Z. Yang, J. Qian, K. Li, W. Shao, H. Zhang, W. Xu, and Q. Liu. Solving motion\\nplanning tasks with a scalable generative model. In European Conference on Computer Vision,\\n2024.\\n[82] B. Huang, Y. Wang, X. Yang, Y. Luo, and Y. Li. 3d-vitac: Learning fine-grained manipulation with\\nvisuo-tactile sensing. In Conference on Robot Learning, 2024.\\n[83] J. Huang and K. C.-C. Chang. Towards reasoning in large language models: A survey. ArXiv,\\nabs/2212.10403, 2022.\\n[84] W. Huang, C. Wang, Y. Li, R. Zhang, and L. Fei-Fei. Rekep: Spatio-temporal reasoning of\\nrelational keypoint constraints for robotic manipulation. In Conference on Robot Learning, 2024.\\n[85] Y. Huang, W. Zheng, B. Zhang, J. Zhou, and J. Lu. Selfocc: Self-supervised vision-based 3d\\noccupancy prediction. In Conference on Computer Vision and Pattern Recognition, 2024.\\n[86] Z. Huang, T. Tang, S. Chen, S. Lin, Z. Jie, L. Ma, G. Wang, and X. Liang*. Making large language\\nmodels better planners with reasoning-decision alignment. In European Conference on Computer\\nVision, 2024.\\n[87] J. Hübotter, S. Bongni, I. Hakimi, and A. Krause. Efficiently learning at test-time: Active fine-tuning\\nof llms. In International Conference on Learning Representations, 2025.\\n[88] T. Ingebrand, A. Thorpe, and U. Topcu. Zero-shot transfer of neural odes. In Neural Information\\nProcessing Systems, 2024.\\n[89] S. Jain, E. S. Lubana, K. Oksuz, T. Joy, P. Torr, A. Sanyal, and P. K. Dokania. What makes and\\nbreaks safety fine-tuning? a mechanistic study. In Neural Information Processing Systems, 2024.\\n[90] U. Jain, L. Weihs, E. Kolve, A. Farhadi, S. Lazebnik, A. Kembhavi, and A. Schwing. A cordial\\nsync: Going beyond marginal policies for multi-agent embodied tasks. In European Conference on\\nComputer Vision, 2024.\\n[91] V. Jain, M. Attarian, N. J. Joshi, A. Wahid, D. Driess, Q. Vuong, P. R. Sanketi, P. Sermanet,\\nS. Welker, C. Chan, I. Gilitschenski, Y. Bisk, and D. Dwibedi. Vid2robot: End-to-end video-\\nconditioned policy learning with cross-attention transformers. In Robotics: Science and Systems,\\n2024.\\n[92] B. Jiang, X. Chen, C. Zhang, F. Yin, Z. Li, G. Yu, and J. Fan*. Motionchain: Conversational\\nmotion controllers via multimodal prompts. In European Conference on Computer Vision, 2024.\\n[93] K. Jiang, J. Huang, W. Xie, J. Lei, Y. Li, L. Shao, and S. Lu. Da-bev: Unsupervised domain\\nadaptation for bird’s eye view perception. In European Conference on Computer Vision, 2024.\\n[94] H. Jin, H. Jiang, H. Tan, K. Zhang, S. Bi, T. Zhang, F. Luan, N. Snavely, and Z. Xu. Lvsm: A large\\nview synthesis model with minimal 3d inductive bias. In International Conference on Learning\\nRepresentations, 2025.\\n[95] Y. Ju, K. Hu, G. Zhang, G. Zhang, M. Jiang, and H. Xu*. Robo-abc: Affordance generalization\\nbeyond categories via semantic correspondence for robot manipulation. In European Conference\\non Computer Vision, 2024.\\n39'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 39}, page_content='[96] J. Kang, L. Karlinsky, H. Luo, Z. Wang, J. A. Hansen, J. R. Glass, D. D. Cox, R. Panda, R. Feris,\\nand A. Ritter. Self-moe: Towards compositional large language models with self-specialized\\nexperts. In International Conference on Learning Representations, 2025.\\n[97] U. Katz, M. Levy, and Y. Goldberg. Knowledge navigator: Llm-guided browsing framework for\\nexploratory search in scientific literature. arXiv preprint arXiv:2408.15836, 2024.\\n[98] T.-W. Ke, N. Gkanatsios, and K. Fragkiadaki. 3d diffuser actor: Policy diffusion with 3d scene\\nrepresentations. In Conference on Robot Learning, 2024.\\n[99] T. Kehrenberg, M. Bartlett, O. Thomas, and N. Quadrianto. Null-sampling for interpretable and\\nfair representations. In European Conference on Computer Vision, 2024.\\n[100] M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. P. Foster,\\nP. R. Sanketi, Q. Vuong, T. Kollar, B. Burchfiel, R. Tedrake, D. Sadigh, S. Levine, P. Liang,\\nand C. Finn. Openvla: An open-source vision-language-action model. In Conference on Robot\\nLearning, 2024.\\n[101] S. Kim*, B. Jeong, D. Kim, and S. Kwak*. Efficient and versatile robust fine-tuning of zero-shot\\nmodels. In European Conference on Computer Vision, 2024.\\n[102] T. Kim, Y. Kwon, J. Lee, T. Kim, and S. Ha. Cprune: Compiler-informed model pruning for\\nefficient target-aware dnn execution. In European Conference on Computer Vision, 2024.\\n[103] J. Krantz and S. Lee. Sim-2-sim transfer for vision-and-language navigation in continuous environ-\\nments. In European Conference on Computer Vision, 2024.\\n[104] J. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee. Beyond the nav-graph: Vision-and-\\nlanguage navigation in continuous environments. In European Conference on Computer Vision,\\n2024.\\n[105] M. Krenn and A. Zeilinger. Predicting research trends with semantic and neural networks with\\nan application in quantum physics. Proceedings of the National Academy of Sciences, 117(4):\\n1910–1916, 2020.\\n[106] M. Krenn, L. Buffoni, B. Coutinho, S. Eppel, J. G. Foster, A. Gritsevskiy, H. Lee, Y. Lu, J. P.\\nMoutinho, N. Sanjabi, et al. Forecasting the future of artificial intelligence with machine learning-\\nbased link prediction in an exponentially growing knowledge network. Nature Machine Intelligence,\\n5(11):1326–1335, 2023.\\n[107] N. Kumar, T. Silver, W. McClinton, L. Zhao, S. Proulx, T. Lozano-Pérez, L. P. Kaelbling, and J. L.\\nBarry. Practice makes perfect: Planning to learning skill parameter policies. In Robotics: Science\\nand Systems, 2024.\\n[108] W. Kuo, F. Bertsch, W. Li, A. Piergiovanni, M. Saffar, and A. Angelova. Findit: Generalized\\nlocalization with natural language queries. In European Conference on Computer Vision, 2024.\\n[109] C.-M. Lai, H.-C. Wang, P.-C. Hsieh, Y.-C. F. Wang, M.-H. Chen, and S.-H. Sun. Diffusion-reward\\nadversarial imitation learning. In Neural Information Processing Systems, 2024.\\n[110] M. Le, C. Nguyen, H. Nguyen, Q. Tran, T. Le, and N. Ho. Revisiting prefix-tuning: Statis-\\ntical benefits of reparameterization among prompts. In International Conference on Learning\\nRepresentations, 2025.\\n40'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 40}, page_content='[111] S. Lee, S. H. Park, S. Kim, and M. Seo. Aligning to thousands of preferences via system message\\ngeneralization. In Neural Information Processing Systems, 2024.\\n[112] F. Li, R. Zhang, H. Zhang, Y. Zhang, B. Li, W. Li, Z. MA, and C. Li. Llava-interleave: Tackling\\nmulti-image, video, and 3d in large multimodal models. In International Conference on Learning\\nRepresentations, 2025.\\n[113] K. Li, J. Wang, L. Yang, C. Lu, and B. Dai. Semgrasp: Semantic grasp generation via language\\naligned discretization. In European Conference on Computer Vision, 2024.\\n[114] P. Li, Z. Wang, X. Zhang, R. Zhang, L. Jiang, P. Wang, and Y. Zhou. Scitopic: Enhancing topic\\ndiscovery in scientific literature through advanced llm. arXiv preprint arXiv:2508.20514, 2025.\\n[115] S. Li, J. Huang, J. Zhuang, Y. Shi, X. Cai, M. Xu, X. Wang, L. Zhang, G. Ke, and H. Cai. Scilitllm:\\nHow to adapt llms for scientific literature understanding. arXiv preprint arXiv:2408.15545, 2024.\\n[116] W. Li, P. Wan, P. Wang, J. Li, Y. Zhou, and P. Liu*. Benerf:neural radiance fields from a single\\nblurry image and event stream. In European Conference on Computer Vision, 2024.\\n[117] X. Li, K. Hsu, J. Gu, O. Mees, K. Pertsch, H. R. Walke, C. Fu, I. Lunawat, I. Sieh, S. Kirmani,\\nS. Levine, J. Wu, C. Finn, H. Su, Q. Vuong, and T. Xiao. Evaluating real-world robot manipulation\\npolicies in simulation. In Conference on Robot Learning, 2024.\\n[118] X. Li, C. Herrmann, K. C. Chan, Y. Li, D. Sun, C. Ma, and M.-H. Yang. A simple approach\\nto unifying diffusion-based conditional generation. In International Conference on Learning\\nRepresentations, 2025.\\n[119] Y. Li, Y. Deng, J. Zhang, J. Jang, M. Memmel, C. R. Garrett, F. Ramos, D. Fox, A. Li, A. Gupta, and\\nA. Goyal. Hamster: Hierarchical action models for open-world robot manipulation. In International\\nConference on Learning Representations, 2025.\\n[120] Z. Li, W. Wang, H. Li, E. Xie, C. Sima, T. Lu, Y. Qiao, and J. Dai. Bevformer: Learning bird’s-\\neye-view representation from multi-camera images via spatiotemporal transformers. In European\\nConference on Computer Vision, 2024.\\n[121] J. Liang, L. Jiang, and A. Hauptmann. Simaug: Learning robust representations from simulation\\nfor trajectory prediction. In European Conference on Computer Vision, 2024.\\n[122] J. Liang, F. Xia, W. Yu, A. Zeng, M. Attarian, M. B. Villalonga, M. Bennice, A. Bewley, A. Dost-\\nmohamed, C. Fu, N. Gileadi, M. Giustina, K. Gopalakrishnan, L. Hasenclever, J. Humplik, J. Hsu,\\nN. J. Joshi, B. Jyenis, J. C. Kew, S. Kirmani, T.-W. E. Lee, K.-H. Lee, A. H. Michaely, J. Moore,\\nK. Oslund, D. Rao, A. Z. Ren, B. Tabanpour, Q. Vuong, A. Wahid, T. Xiao, Y. Xu, V. Zhuang, P. Xu,\\nE. Frey, K. Caluwaerts, T. Zhang, B. Ichter, J. Tompson, L. Takayama, V. Vanhoucke, I. Shafran,\\nM. Mataric, D. Sadigh, N. Heess, K. Rao, N. Stewart, J. Tan, and C. Parada. Learning to learn\\nfaster from human feedback with language model predictive control. In Robotics: Science and\\nSystems, 2024.\\n[123] M. Liang, B. Yang, S. Wang, and R. Urtasun. Deep continuous fusion for multi-sensor 3d object\\ndetection. In European Conference on Computer Vision, 2024.\\n[124] W. Liang, Y. Zhang, H. Cao, B. Wang, D. Y. Ding, X. Yang, K. Vodrahalli, S. He, D. S. Smith,\\nY. Yin, et al. Can large language models provide useful feedback on research papers? a large-scale\\nempirical analysis. NEJM AI, 1(8):AIoa2400196, 2024.\\n41'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 41}, page_content='[125] Y. Liang, K. Ellis, and J. Henriques. Rapid motor adaptation for robotic manipulator arms. In\\nConference on Computer Vision and Pattern Recognition, 2024.\\n[126] Y. Liang, X. Fang, H. Chen, and Y. Wang. Linear multistep solver distillation for fast sampling of\\ndiffusion models. In International Conference on Learning Representations, 2025.\\n[127] Z. Liang, Y. Mu, H. Ma, M. Tomizuka, M. Ding, and P. Luo. Skilldiffuser: Interpretable hierarchical\\nplanning via skill abstractions in diffusion-based task execution. In Conference on Computer Vision\\nand Pattern Recognition, 2024.\\n[128] B. LiChen, S. Shao, zikai zhou, Z. Qi, zhiqiang xu, H. Xiong, and Z. Xie. Zigzag diffusion\\nsampling: Diffusion models can self-improve via self-reflection. In International Conference on\\nLearning Representations, 2025.\\n[129] C.-H. Lin, S. Gao, J. S. Smith, A. Patel, S. Tuli, Y. Shen, H. Jin, and Y.-C. Hsu. Modegpt: Modular\\ndecomposition for large language model compression. In International Conference on Learning\\nRepresentations, 2025.\\n[130] H. Lin, J. Cho, A. Zala, and M. Bansal. Ctrl-adapter: An efficient and versatile framework\\nfor adapting diverse controls to any diffusion model. In International Conference on Learning\\nRepresentations, 2025.\\n[131] Z. Lin, X. Peng, P. Cong, G. Zheng, Y. Sun, Y. HOU, X. Zhu, S. Yang, and Y. Ma*. Wildrefer: 3d\\nobject localization in large-scale dynamic scenes with multi-modal visual data and natural language.\\nIn European Conference on Computer Vision, 2024.\\n[132] V. Lingam, A. T. Neerkaje, A. Vavre, A. Shetty, G. K. Gudur, J. Ghosh, E. Choi, A. Dimakis,\\nA. Bojchevski, and S. Sanghavi. Svft: Parameter-efficient fine-tuning with singular vectors. In\\nNeural Information Processing Systems, 2024.\\n[133] H. Liu, Y. Chen, H. Wang, Z. Yang, T. Li, J. Zeng, L. Chen, H. Li, and L. Wang. Fully sparse 3d\\noccupancy prediction. In European Conference on Computer Vision, 2024.\\n[134] H. Liu, Y. Zhang, V. Betala, E. Zhang, J. Liu, C. Ding, and Y. Zhu. Multi-task interactive robot\\nfleet learning with visual world models. In Conference on Robot Learning, 2024.\\n[135] M. Liu, Z. Chen, X. Cheng, Y. Ji, R.-Z. Qiu, R. Yang, and X. Wang. Visual whole-body control for\\nlegged loco-manipulation. In Conference on Robot Learning, 2024.\\n[136] N. Liu, S. Li, Y. Du, A. Torralba, and J. B. Tenenbaum. Compositional visual generation with\\ncomposable diffusion models. In European Conference on Computer Vision, 2024.\\n[137] S. Liu*, H. Cheng, H. Liu, H. Zhang, F. Li, T. Ren, X. Zou, J. Yang, H. Su, J. Zhu, L. Zhang,\\nJ. Gao, and C. Li*. Llava-plus: Learning to use tools for creating multimodal agents. In European\\nConference on Computer Vision, 2024.\\n[138] S. Liu, J. Nam, A. Campbell, H. Stark, Y. Xu, T. Jaakkola, and R. Gomez-Bombarelli. Think while\\nyou generate: Discrete diffusion with planned denoising. In International Conference on Learning\\nRepresentations, 2025.\\n[139] S. Liu, L. Wu, B. Li, H. Tan, H. Chen, Z. Wang, K. Xu, H. Su, and J. Zhu. Rdt-1b: a diffusion foun-\\ndation model for bimanual manipulation. In International Conference on Learning Representations,\\n2025.\\n42'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 42}, page_content='[140] W. Liu, Z. Liu, L. Paull, A. Weller, and B. Schölkopf. Structural causal 3d reconstruction. In\\nEuropean Conference on Computer Vision, 2024.\\n[141] Y. Liu, Y. Zhang, Y. Wang, F. Hou, J. Yuan, J. Tian, Y. Zhang, Z. Shi, J. Fan, and Z. He. A survey\\nof visual transformers. IEEE transactions on neural networks and learning systems, 2023.\\n[142] Z. Liu, M. Lu, S. Zhang, B. Liu, H. Guo, Y. Yang, J. Blanchet, and Z. Wang. Provably mitigating\\noveroptimization in rlhf: Your sft loss is implicitly an adversarial regularizer. In Neural Information\\nProcessing Systems, 2024.\\n[143] V. S. Lokhande, A. K. Akash, S. N. Ravi, and V. Singh. Fairalm: Augmented lagrangian method\\nfor training fair models with little regret. In European Conference on Computer Vision, 2024.\\n[144] J. Long, W. Yu, Q. Li, Z. Wang, D. Lin, and J. Pang. Learning h-infinity locomotion control. In\\nConference on Robot Learning, 2024.\\n[145] C. Lu, C. Lu, R. T. Lange, J. Foerster, J. Clune, and D. Ha. The ai scientist: Towards fully\\nautomated open-ended scientific discovery. arXiv preprint arXiv:2408.06292, 2024.\\n[146] T. G. W. Lum, M. Matak, V. Makoviychuk, A. Handa, A. Allshire, T. Hermans, N. D. Ratliff, and\\nK. V. Wyk. Dextrah-g: Pixels-to-action dexterous arm-hand grasping with geometric fabrics. In\\nConference on Robot Learning, 2024.\\n[147] G. Luo, T. Darrell, O. Wang, D. B. Goldman, and A. Holynski. Readout guidance: Learning control\\nfrom diffusion features. In Conference on Computer Vision and Pattern Recognition, 2024.\\n[148] J. Luo, T. Ding, K. H. R. Chan, D. Thaker, A. Chattopadhyay, C. Callison-Burch, and R. Vidal. Pace:\\nParsimonious concept engineering for large language models. In Neural Information Processing\\nSystems, 2024.\\n[149] X. Luo, A. Rechardt, G. Sun, K. K. Nejad, F. Yáñez, B. Yilmaz, K. Lee, A. O. Cohen, V. Borghesani,\\nA. Pashkov, et al. Large language models surpass human experts in predicting neuroscience results.\\nNature human behaviour, 9(2):305–315, 2025.\\n[150] J. Lyu, M. Yan, Z. Qiao, R. Liu, X. Ma, D. Ye, J.-W. Yang, Z. Lu, and X. Li. Cross-domain offline\\npolicy adaptation with optimal transport and dataset constraint. In International Conference on\\nLearning Representations, 2025.\\n[151] K. Lyu, H. Zhao, X. Gu, D. Yu, A. Goyal, and S. Arora. Keeping llms aligned after fine-tuning:\\nThe crucial role of prompt templates. In Neural Information Processing Systems, 2024.\\n[152] X. Ma, S. Patidar, I. Haughton, and S. James. Hierarchical diffusion policy for kinematics-aware\\nmulti-task robotic manipulation. In Conference on Computer Vision and Pattern Recognition, 2024.\\n[153] Y. Ma, Z. Song, Y. Zhuang, J. Hao, and I. King. A survey on vision-language-action models for\\nembodied ai. arXiv preprint arXiv:2405.14093, 2024.\\n[154] D. Mahajan, R. Girshick, V. Ramanathan, K. He, M. Paluri, Y. Li, A. Bharambe, and L. van der\\nMaaten. Exploring the limits of weakly supervised pretraining. In European Conference on\\nComputer Vision, 2024.\\n[155] B. P. Majumder, H. Surana, D. Agarwal, B. D. Mishra, A. Meena, A. Prakhar, T. Vora, T. Khot,\\nA. Sabharwal, and P. Clark. Discoverybench: Towards data-driven discovery with large language\\nmodels. arXiv preprint arXiv:2407.01725, 2024.\\n43'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 43}, page_content='[156] B. S. Manning, K. Zhu, and J. J. Horton. Automated social science: Language models as scientist\\nand subjects. Technical report, National Bureau of Economic Research, 2024.\\n[157] E. Margffoy-Tuay, J. C. Perez, E. Botero, and P. Arbelaez. Dynamic multimodal instance seg-\\nmentation guided by natural language queries. In European Conference on Computer Vision,\\n2024.\\n[158] P. Mazzaglia, T. Verbelen, B. Dhoedt, A. Courville, and S. Rajeswar. Genrl: Multimodal-foundation\\nworld models for generalization in embodied agents. In Neural Information Processing Systems,\\n2024.\\n[159] F. Meng, Z. Wang, and M. Zhang. Pissa: Principal singular values and singular vectors adaptation\\nof large language models. In Neural Information Processing Systems, 2024.\\n[160] O.-B. Mercea, A. Gritsenko, C. Schmid, and A. Arnab. Time- memory- and parameter-efficient\\nvisual adaptation. In Conference on Computer Vision and Pattern Recognition, 2024.\\n[161] L. Messeri and M. J. Crockett. Artificial intelligence and illusions of understanding in scientific\\nresearch. Nature, 627(8002):49–58, 2024.\\n[162] J. Michaux, A. Li, Q. Chen, C. Chen, and R. Vasudevan. Safe planning for articulated robots using\\nreachability-based obstacle avoidance with spheres. In Robotics: Science and Systems, 2024.\\n[163] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf:\\nRepresenting scenes as neural radiance fields for view synthesis. In European Conference on\\nComputer Vision, 2024.\\n[164] S. Mo, F. Mu, K. H. Lin, Y. Liu, B. Guan, Y. Li, and B. Zhou. Freecontrol: Training-free spatial\\ncontrol of any text-to-image diffusion model with any condition. In Conference on Computer\\nVision and Pattern Recognition, 2024.\\n[165] W. Mo, T. Zhang, Y. Bai, B. Su, J.-R. Wen, and Q. Yang. Dynamic prompt optimizing for\\ntext-to-image generation. In Conference on Computer Vision and Pattern Recognition, 2024.\\n[166] T. Mu, A. Helyar, J. Heidecke, J. Achiam, A. Vallone, I. D. Kivlichan, M. Lin, A. Beutel, J. Schul-\\nman, and L. Weng. Rule based rewards for language model safety. In Neural Information Processing\\nSystems, 2024.\\n[167] M. Najibi, J. Ji, Y. Zhou, C. R. Qi, X. Yan, S. Ettinger, and D. Anguelov. Motion inspired\\nunsupervised perception and prediction in autonomous driving. In European Conference on\\nComputer Vision, 2024.\\n[168] M. Narasimhan, E. Wijmans, X. Chen, T. Darrell, D. Batra, D. Parikh, and A. Singh. Seeing\\nthe un-scene: Learning amodal semantic maps for room navigation. In European Conference on\\nComputer Vision, 2024.\\n[169] X. Ning, T. Zhao, W. Li, P. Lei, Y. Wang, and H. Yang. Dsa: More efficient budgeted pruning via\\ndifferentiable sparsity allocation. In European Conference on Computer Vision, 2024.\\n[170] D. Niu, Y. Sharma, G. Biamby, J. Quenum, Y. Bai, B. Shi, T. Darrell, and R. Herzig. Llarva:\\nVision-action instruction tuning enhances robot learning. In Conference on Robot Learning, 2024.\\n[171] A. Oshin, H. Almubarak, and E. Theodorou. Differentiable robust model predictive control. In\\nRobotics: Science and Systems, 2024.\\n44'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 44}, page_content='[172] P. Oza, H. V. Nguyen, and V. M. Patel. Multiple class novelty detection under data distribution\\nshift. In European Conference on Computer Vision, 2024.\\n[173] C. Pan, Z. Yi, G. Shi, and G. Qu. Model-based diffusion for trajectory optimization. In Neural\\nInformation Processing Systems, 2024.\\n[174] R. Parihar, S. VS, S. Mani, T. Karmali, and V. B. Radhakrishnan. Precisecontrol: Enhancing\\ntext-to-image diffusion models with fine-grained attribute control. In European Conference on\\nComputer Vision, 2024.\\n[175] L. Peng, J. Xu, H. Cheng, Z. Yang, X. Wu, W. Qian, W. Wang, B. Wu, and D. Cai. Learning\\noccupancy for monocular 3d object detection. In Conference on Computer Vision and Pattern\\nRecognition, 2024.\\n[176] M. Peychev, A. Ruoss, M. Balunovi´c, M. Baader, and M. Vechev. Latent space smoothing for\\nindividually fair representations. In European Conference on Computer Vision, 2024.\\n[177] C. M. Pham, A. Hoyle, S. Sun, P. Resnik, and M. Iyyer. Topicgpt: A prompt-based topic modeling\\nframework. arXiv preprint arXiv:2311.01449, 2023.\\n[178] S. Purushwalkam, P. Morgado, and A. Gupta. The challenges of continuous self-supervised learning.\\nIn European Conference on Computer Vision, 2024.\\n[179] Y. Qi, Z. Pan, S. Zhang, A. van den Hengel, and Q. Wu. Object-and-action aware model for visual\\nlanguage navigation. In European Conference on Computer Vision, 2024.\\n[180] Z. Qi, R. Dong, S. Zhang, H. Geng, C. Han, Z. Ge, L. Yi*, and K. Ma*. Shapellm: Universal 3d\\nobject understanding for embodied interaction. In European Conference on Computer Vision, 2024.\\n[181] C. Qian, Z. Xie, Y. Wang, W. Liu, K. Zhu, H. Xia, Y. Dang, Z. Du, W. Chen, C. Yang, Z. Liu,\\nand M. Sun. Scaling large language model-based multi-agent collaboration. In International\\nConference on Learning Representations, 2025.\\n[182] L. Rout, Y. Chen, N. Ruiz, C. Caramanis, S. Shakkottai, and W.-S. Chu. Semantic image inversion\\nand editing using rectified stochastic differential equations. In International Conference on Learning\\nRepresentations, 2025.\\n[183] D. Rozenberszki, O. Litany, and A. Dai. Language-grounded indoor 3d semantic segmentation in\\nthe wild. In European Conference on Computer Vision, 2024.\\n[184] H. Ryu, S. Lim, and H. Shim. Memory-efficient fine-tuning for quantized diffusion model. In\\nEuropean Conference on Computer Vision, 2024.\\n[185] A. Sadat, S. Casas, M. Ren, X. Wu, P. Dhawan, and R. Urtasun. Perceive, predict, and plan:\\nSafe motion planning through interpretable semantic representations. In European Conference on\\nComputer Vision, 2024.\\n[186] T. Salimans, T. Mensink, J. Heek, and E. Hoogeboom. Multistep distillation of diffusion models\\nvia moment matching. In Neural Information Processing Systems, 2024.\\n[187] A. Sampieri, G. M. D. di Melendugno, A. Avogaro, F. Cunico, F. Setti, G. Skenderi, M. Cristani,\\nand F. Galasso. Pose forecasting in industrial human-robot collaboration. In European Conference\\non Computer Vision, 2024.\\n[188] M. H. Sarhan, N. Navab, A. Eslami, and S. Albarqouni. Fairness by learning orthogonal disentan-\\ngled representations. In European Conference on Computer Vision, 2024.\\n45'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 45}, page_content='[189] Y. Savani, M. A. Finzi, and J. Z. Kolter. Diffusing differentiable representations. In Neural\\nInformation Processing Systems, 2024.\\n[190] S. Schmidgall, Y. Su, Z. Wang, X. Sun, J. Wu, X. Yu, J. Liu, M. Moor, Z. Liu, and E. Barsoum.\\nAgent laboratory: Using llm agents as research assistants. arXiv preprint arXiv:2501.04227, 2025.\\n[191] Y. Shaoul, I. Mishani, S. Vats, J. Li, and M. Likhachev. Multi-robot motion planning with diffusion\\nmodels. In International Conference on Learning Representations, 2025.\\n[192] R. Shetty, M. Fritz, and B. Schiele. Towards automated testing and robustification by semantic\\nadversarial data generation. In European Conference on Computer Vision, 2024.\\n[193] F. Shi, C. Zhang, T. Miki, J. Lee, M. Hutter, and S. Coros. Rethinking robustness assessment:\\nAdversarial attacks on learning-based quadrupedal locomotion controllers. In Robotics: Science\\nand Systems, 2024.\\n[194] L. X. Shi, Z. Hu, T. Z. Zhao, A. Sharma, K. Pertsch, J. Luo, S. Levine, and C. Finn. Yell at your\\nrobot: Improving on-the-fly from language corrections. In Robotics: Science and Systems, 2024.\\n[195] M. Shi, F. Liu, S. Wang, S. Liao, S. Radhakrishnan, Y. Zhao, D.-A. Huang, H. Yin, K. Sapra,\\nY. Yacoob, H. Shi, B. Catanzaro, A. Tao, J. Kautz, Z. Yu, and G. Liu. Eagle: Exploring the design\\nspace for multimodal llms with mixture of encoders. In International Conference on Learning\\nRepresentations, 2025.\\n[196] X. Shi, Y. Li, Q. Kou, L. Yu, J. Xie, and H. Zhou. Spar: Scholar paper retrieval with llm-based\\nagents for enhanced academic search. arXiv preprint arXiv:2507.15245, 2025.\\n[197] C. Si, X. Wang, X. Yang, Z. Xu, Q. Li, J. Dai, Y. Qiao, X. Yang, and W. Shen. Maintaining structural\\nintegrity in parameter spaces for parameter efficient fine-tuning. In International Conference on\\nLearning Representations, 2025.\\n[198] R. Sinha, A. Elhafsi, C. Agia, M. Foutter, E. Schmerling, and M. Pavone. Real-time anomaly\\ndetection and reactive planning with large language models. In Robotics: Science and Systems,\\n2024.\\n[199] S. Skand, B. Pandit, C. Kim, L. Fuxin, and S. Lee. Simple masked training strategies yield control\\npolicies that are robust to sensor failure. In Conference on Robot Learning, 2024.\\n[200] J. P. Sleiman, M. Mittal, and M. Hutter. Guided reinforcement learning for robust multi-contact\\nloco-manipulation. In Conference on Robot Learning, 2024.\\n[201] H. Song, W. Ding, Y. Chen, S. Shen, M. Y. Wang, and Q. Chen. Pip: Planning-informed trajectory\\nprediction for autonomous driving. In European Conference on Computer Vision, 2024.\\n[202] J. Song, Y. Yang, H. Xiao, W. Peng, W. Yao, and F. Wang. Laser: Towards diversified and\\ngeneralizable robot design with large language models. In International Conference on Learning\\nRepresentations, 2025.\\n[203] V. Sreeramdass, R. R. Paleja, L. Chen, S. van Waveren, and M. Gombolay. Generalized behavior\\nlearning from diverse demonstrations. In International Conference on Learning Representations,\\n2025.\\n[204] S. Srivastava and G. Sharma. Omnivec2 - a novel transformer based network for large scale\\nmultimodal and multitask learning. In Conference on Computer Vision and Pattern Recognition,\\n2024.\\n46'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 46}, page_content='[205] K. Stechly, K. Valmeekam, and S. Kambhampati. On the self-verification limitations of large\\nlanguage models on reasoning and planning tasks. In International Conference on Learning\\nRepresentations, 2025.\\n[206] N. Stracke, S. A. Baumann, J. Susskind, M. A. Bautista, and B. Ommer. Ctrloralter: Conditional\\nloradapter for efficient 0-shot control & altering of t2i models. In European Conference on\\nComputer Vision, 2024.\\n[207] V. Subramaniam, Y. Du, J. B. Tenenbaum, A. Torralba, S. Li, and I. Mordatch. Multiagent\\nfinetuning: Self improvement with diverse reasoning chains. In International Conference on\\nLearning Representations, 2025.\\n[208] O. Taheri, N. Ghorbani, M. J. Black, and D. Tzionas. Grab: A dataset of whole-body human\\ngrasping of objects. In European Conference on Computer Vision, 2024.\\n[209] S. Tan, W. Xiang, H. Liu, D. Guo, and F. Sun. Multi-agent embodied question answering in\\ninteractive environments. In European Conference on Computer Vision, 2024.\\n[210] H. Tang, D. Y. Key, and K. Ellis. Worldcoder, a model-based llm agent: Building world models\\nby writing code and interacting with the environment. In Neural Information Processing Systems,\\n2024.\\n[211] S. Tang*, Y. Wang, C. Ding, Y. Liang, Y. Li, and D. Xu. Adadiff: Accelerating diffusion models\\nthrough step-wise adaptive computation. In European Conference on Computer Vision, 2024.\\n[212] G. Tevet, B. Gordon, A. Hertz, A. H. Bermano, and D. Cohen-Or. Motionclip: Exposing human\\nmotion generation to clip space. In European Conference on Computer Vision, 2024.\\n[213] F. Tian, Y. Li, Y. Yan, S. Guan, Y. Ge, and X. Yang. Postedit: Posterior sampling for efficient\\nzero-shot image editing. In International Conference on Learning Representations, 2025.\\n[214] S. Tong, E. L. B. II, P. Wu, S. Woo, A. J. IYER, S. C. Akula, S. Yang, J. Yang, M. Middepogu,\\nZ. Wang, X. Pan, R. Fergus, Y. LeCun, and S. Xie. Cambrian-1: A fully open, vision-centric\\nexploration of multimodal llms. In Neural Information Processing Systems, 2024.\\n[215] H.-Y. Tseng, M. Fisher, J. Lu, Y. Li, V. Kim, and M.-H. Yang. Modeling artistic workflows for\\nimage generation and editing. In European Conference on Computer Vision, 2024.\\n[216] D. Turpin, L. Wang, E. Heiden, Y.-C. Chen, M. Macklin, S. Tsogkas, S. Dickinson, and A. Garg.\\nGrasp’d: Differentiable contact-rich grasp synthesis for multi-fingered hands. In European Confer-\\nence on Computer Vision, 2024.\\n[217] R. Van Noorden and J. M. Perkel. Ai and science: what 1,600 researchers think. Nature, 621(7980):\\n672–675, 2023.\\n[218] A. Vettoruzzo, L. Braccaioli, J. Vanschoren, and M. Nowaczyk. Unsupervised meta-learning via\\nin-context learning. In International Conference on Learning Representations, 2025.\\n[219] V. Viswanathan, K. Gashteovski, K. Gashteovski, C. Lawrence, T. Wu, and G. Neubig. Large\\nlanguage models enable few-shot clustering. Transactions of the Association for Computational\\nLinguistics, 12:321–333, 2024.\\n[220] H. Wang, W. Wang, T. Shu, W. Liang, and J. Shen. Active visual information gathering for\\nvision-language navigation. In European Conference on Computer Vision, 2024.\\n47'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 47}, page_content='[221] L. Wang, X. Chen, J. Zhao, and K. He. Scaling proprioceptive-visual learning with heterogeneous\\npre-trained transformers. In Neural Information Processing Systems, 2024.\\n[222] Q. Wang, D. Downey, H. Ji, and T. Hope. Scimon: Scientific inspiration machines optimized\\nfor novelty. In Proceedings of the 62nd Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), pages 279–299, 2024.\\n[223] R. Wang, J. Xiang, J. Yang, and X. Tong. Diffusion models are geometry critics: Single image 3d\\nediting using pre-trained diffusion priors. In European Conference on Computer Vision, 2024.\\n[224] T.-H. Wang, S. Manivasagam, M. Liang, B. Yang, W. Zeng, and R. Urtasun. V2vnet: Vehicle-to-\\nvehicle communication for joint perception and prediction. In European Conference on Computer\\nVision, 2024.\\n[225] W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, S. XiXuan, J. Xu,\\nK. Chen, B. Xu, J. Li, Y. Dong, M. Ding, and J. Tang. Cogvlm: Visual expert for pretrained\\nlanguage models. In Neural Information Processing Systems, 2024.\\n[226] X. Wang*, Y. Zhang, O. Zohar, and S. Yeung-Levy. Videoagent: Long-form video understanding\\nwith large language model as agent. In European Conference on Computer Vision, 2024.\\n[227] X. Wang, Z. Zhu, G. Huang, C. Xinze, J. Zhu, and J. Lu. Drivedreamer: Towards real-world-driven\\nworld models for autonomous driving. In European Conference on Computer Vision, 2024.\\n[228] X. E. Wang, V. Jain, E. Ie, W. Y. Wang, Z. Kozareva, and S. Ravi. Environment-agnostic multitask\\nlearning for natural language grounded navigation. In European Conference on Computer Vision,\\n2024.\\n[229] Y. Wang, J. He, L. Fan, H. Li, Y. Chen, and Z. Zhang. Driving into the future: Multiview visual\\nforecasting and planning with world model for autonomous driving. In Conference on Computer\\nVision and Pattern Recognition, 2024.\\n[230] Y. Wang*, K. Li, X. Li, J. Yu, Y. He, G. Chen, B. Pei, R. Zheng, J. Xu, Z. Wang, Y. Shi, T. Jiang,\\nS. Li, hongjie Zhang, Y. Huang, Y. Qiao*, Y. Wang*, and L. Wang*. Internvideo2: Scaling\\nfoundation models for multimodal video understanding. In European Conference on Computer\\nVision, 2024.\\n[231] Y. Wang, Y. Lu, and T. Blankevoort. Differentiable joint pruning and quantization for hardware\\nefficiency. In European Conference on Computer Vision, 2024.\\n[232] Y. Wang, C. Tang, L. Sun, S. Rossi, Y. Xie, C. Peng, T. Hannagan, S. Sabatini, N. Poerio,\\nM. Tomizuka, and W. Zhan. Optimizing diffusion models for joint trajectory prediction and\\ncontrollable generation. In European Conference on Computer Vision, 2024.\\n[233] Z. Wang, Z. Zhang, S. Ebrahimi, R. Sun, H. Zhang, C.-Y. Lee, X. Ren, G. Su, V. Perot, J. Dy,\\nand T. Pfister. Dualprompt: Complementary prompting for rehearsal-free continual learning. In\\nEuropean Conference on Computer Vision, 2024.\\n[234] Z. Wang, Z. Liu, T. Ma, J. Li, Z. Zhang, X. Fu, Y. Li, Z. Yuan, W. Song, Y. Ma, et al. Graph\\nfoundation models: A comprehensive survey. arXiv preprint arXiv:2505.15116, 2025.\\n[235] A. Werby, C. Huang, M. Büchner, A. Valada, and W. Burgard. Hierarchical open-vocabulary 3d\\nscene graphs for language-grounded robot navigation. In Robotics: Science and Systems, 2024.\\n48'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 48}, page_content='[236] M. Williams, M. Carroll, A. Narang, C. Weisser, B. Murphy, and A. Dragan. On targeted manip-\\nulation and deception when optimizing llms for user feedback. In International Conference on\\nLearning Representations, 2025.\\n[237] K. Wong, Q. Zhang, M. Liang, B. Yang, R. Liao, A. Sadat, and R. Urtasun. Testing the safety\\nof self-driving vehicles by simulating perception and prediction. In European Conference on\\nComputer Vision, 2024.\\n[238] S. Wu, H. Fei, X. Li, J. Ji, H. Zhang, T.-S. Chua, and S. YAN. Towards semantic equivalence of\\ntokenization in multimodal llm. In International Conference on Learning Representations, 2025.\\n[239] Y. Wu, J. Wang, Y. Zhang, S. Zhang, O. Hilliges, F. Yu, and S. Tang. Saga: Stochastic whole-body\\ngrasping with contact. In European Conference on Computer Vision, 2024.\\n[240] Y. Wu, Z. Sun, H. Yuan, K. Ji, Y. Yang, and Q. Gu. Self-play preference optimization for language\\nmodel alignment. In International Conference on Learning Representations, 2025.\\n[241] Y. Wu, Z. Zhang, J. Chen, H. Tang, D. Li, Y. Fang, L. Zhu, E. Xie, H. Yin, L. Yi, S. Han, and\\nY. Lu. Vila-u: a unified foundation model integrating visual understanding and generation. In\\nInternational Conference on Learning Representations, 2025.\\n[242] X. Xiao, J. Liu, Z. Wang, Y. Zhou, Y. Qi, S. Jiang, B. He, and Q. Cheng. Robot learning in the era\\nof foundation models: A survey. Neurocomputing, page 129963, 2025.\\n[243] J. Xing*, M. Xia, Y. Zhang, H. Chen, W. Yu, H. Liu, G. Liu, X. Wang, Y. Shan, and T.-T.\\nWong. Dynamicrafter: Animating open-domain images with video diffusion priors. In European\\nConference on Computer Vision, 2024.\\n[244] D. Xu, Y. Jiang, P. Wang, Z. Fan, H. Shi, and Z. Wang. Sinnerf: Training neural radiance fields on\\ncomplex scenes from a single image. In European Conference on Computer Vision, 2024.\\n[245] F. F. Xu, U. Alon, G. Neubig, and V. J. Hellendoorn. A systematic evaluation of large language\\nmodels of code. In Proceedings of the 6th ACM SIGPLAN international symposium on machine\\nprogramming, pages 1–10, 2022.\\n[246] Z. Xu, K. Wu, J. Wen, J. Li, N. Liu, Z. Che, and J. Tang. A survey on robotics with foundation\\nmodels: toward embodied ai. arXiv preprint arXiv:2402.02385, 2024.\\n[247] Z. Xu, M. Liu, Y. Shen, J. Rimchala, J. Zhang, Q. Wang, Y. Cheng, and L. Huang. Modality-\\nspecialized synergizers for interleaved vision-language generalists. In International Conference on\\nLearning Representations, 2025.\\n[248] F. Yang, C. Feng, Z. Chen, H. Park, D. Wang, Y. Dou, Z. Zeng, X. Chen, R. Gangopadhyay,\\nA. Owens, and A. Wong. Binding touch to everything: Learning unified multimodal tactile\\nrepresentations. In Conference on Computer Vision and Pattern Recognition, 2024.\\n[249] M. Yang, C. Lu, A. Church, Y. Lin, C. J. Ford, H. Li, E. Psomopoulou, D. A. Barton, and N. F.\\nLepora. Anyrotate: Gravity-invariant in-hand object rotation with sim-to-real touch. In Conference\\non Robot Learning, 2024.\\n[250] R. Yang, Z. Chen, J. Ma, C. Zheng, Y. Chen, Q. Nguyen, and X. Wang. Generalized animal imitator:\\nAgile locomotion with versatile motion prior. In Conference on Robot Learning, 2024.\\n[251] Y. Yang, B. Huang, F. Feng, X. Wang, S. Tu, and L. Xu. Towards generalizable reinforcement\\nlearning via causality-guided self-adaptive representations. In International Conference on Learning\\nRepresentations, 2025.\\n49'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 49}, page_content='[252] S. Yarram* and J. Yuan. Forecasting future videos from novel views via disentangled 3d scene\\nrepresentation. In European Conference on Computer Vision, 2024.\\n[253] J. Ye, J. Gao, S. Gong, L. Zheng, X. Jiang, Z. Li, and L. Kong. Beyond autoregression: Dis-\\ncrete diffusion for complex reasoning and planning. In International Conference on Learning\\nRepresentations, 2025.\\n[254] J. Ye, K. Wang, C. Yuan, R. Yang, Y. Li, J. Zhu, Y. Qin, X. Zou, and X. Wang. Dex1b: Learning\\nwith 1b demonstrations for dexterous manipulation. In Robotics: Science and Systems (RSS), 2025.\\n[255] J. Ye, H. Xu, H. Liu, A. Hu, M. Yan, Q. Qian, J. Zhang, F. Huang, and J. Zhou. mplug-owl3: To-\\nwards long image-sequence understanding in multi-modal large language models. In International\\nConference on Learning Representations, 2025.\\n[256] J. Yin, J. Shen, R. Chen, W. Li, R. Yang, P. Frossard, and W. Wang. Is-fusion: Instance-scene\\ncollaborative fusion for multimodal 3d object detection. In Conference on Computer Vision and\\nPattern Recognition, 2024.\\n[257] P. Yin, T. Westenbroek, C.-A. Cheng, A. Kolobov, and A. Gupta. Rapidly adapting policies\\nto the real-world via simulation-guided fine-tuning. In International Conference on Learning\\nRepresentations, 2025.\\n[258] Z.-H. Yin and P. Abbeel. Offline imitation learning through graph search and retrieval. In Robotics:\\nScience and Systems, 2024.\\n[259] J. Yoon, S. Yu, V. Patil, H. Yao, and M. Bansal. Safree: Training-free and adaptive guard for safe\\ntext-to-image and video generation. In International Conference on Learning Representations,\\n2025.\\n[260] C. Yu, X. Yang, J. Gao, H. Yang, Y. Wang, and Y. Wu. Learning efficient multi-agent cooperative\\nvisual exploration. In European Conference on Computer Vision, 2024.\\n[261] K. Yu, Y. Han, Q. Wang, V. Saxena, D. Xu, and Y. Zhao. Mimictouch: Leveraging multi-modal\\nhuman tactile demonstrations for contact-rich manipulation. In Conference on Robot Learning,\\n2024.\\n[262] S. Yu and C. Lu. Adam: An embodied causal agent in open-world environments. In International\\nConference on Learning Representations, 2025.\\n[263] H. Yuan, B. Zhou, Y. Fu, and Z. Lu. Cross-embodiment dexterous grasping with reinforcement\\nlearning. In International Conference on Learning Representations, 2025.\\n[264] S. Yuan, H. Liu, and H. Xu. Bridging the gap between low-rank and orthogonal adaptation via\\nhouseholder reflection adaptation. In Neural Information Processing Systems, 2024.\\n[265] W. Yuan, J. Duan, V. Blukis, W. Pumacay, R. Krishna, A. Murali, A. Mousavian, and D. Fox.\\nRobopoint: A vision-language model for spatial affordance prediction in robotics. In Conference\\non Robot Learning, 2024.\\n[266] Z. Yuan, T. Wei, S. Cheng, G. Zhang, Y. Chen, and H. Xu. Learning to manipulate anywhere:\\nA visual generalizable framework for reinforcement learning. In Conference on Robot Learning,\\n2024.\\n[267] F. Zeng, W. Gan, Y. Wang, N. Liu, and P. S. Yu. Large language models for robotics: A survey.\\narXiv preprint arXiv:2311.07226, 2023.\\n50'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 50}, page_content='[268] Y. Zhan, Y. Zhu*, Z. Chen, F. Yang, M. Tang, and J. Wang. Griffon: Spelling out all object locations\\nat any granularity with large language models. In European Conference on Computer Vision, 2024.\\n[269] C. Zhang, L. Liu, Y. Cui, G. Huang, W. Lin, Y. Yang, and Y. Hu. A comprehensive survey on\\nsegment anything model for vision and beyond. arXiv preprint arXiv:2305.08196, 2023.\\n[270] G. Zhang, Y. Yue, Z. Li, S. Yun, G. Wan, K. Wang, D. Cheng, J. X. Yu, and T. Chen. Cut the\\ncrap: An economical communication pipeline for llm-based multi-agent systems. In International\\nConference on Learning Representations, 2025.\\n[271] H. Zhang, S. Christen, Z. Fan, O. Hilliges, and J. Song. Graspxl: Generating grasping motions for\\ndiverse objects at scale. In European Conference on Computer Vision, 2024.\\n[272] H. Zhang*, H. Li, F. Li, T. Ren, X. Zou, S. Liu, S. Huang, J. Gao, L. Zhang, C. Li, and J. Yang.\\nLlava-grounding: Grounded visual chat with large multimodal models. In European Conference on\\nComputer Vision, 2024.\\n[273] H. Zhang, D. Tang, J. Liu, M. Lu, J. Zheng, J. Peng, D. Li, Y. Wang, F. Jiang, L. Tian, S. Tiwari,\\nA. Sirasao, J.-H. Yong, B. Wang, and E. Barsoum. Dip-go: A diffusion pruner via few-step gradient\\noptimization. In Neural Information Processing Systems, 2024.\\n[274] H. Zhang, Z. Wang, Q. Lyu, Z. Zhang, S. Chen, T. Shu, B. Dariush, K. Lee, Y. Du, and C. Gan.\\nCombo: Compositional world models for embodied multi-agent cooperation. In International\\nConference on Learning Representations, 2025.\\n[275] J. Zhang, M. Heo, Z. Liu, E. Biyik, J. J. Lim, Y. Liu, and R. Fakoor. Extract: Efficient policy\\nlearning by extracting transferable robot skills from offline data. In Conference on Robot Learning,\\n2024.\\n[276] J. Zhang, K. Wang, R. Xu, G. Zhou, Y. Hong, X. Fang, Q. Wu, Z. Zhang, and H. Wang. Navid:\\nVideo-based vlm plans the next step for vision-and-language navigation. In Robotics: Science and\\nSystems, 2024.\\n[277] J. Zhang, G. Zhu, S. Li, X. Liu, H. Song, X. Tang, and C. Feng. Multiview scene graph. In Neural\\nInformation Processing Systems, 2024.\\n[278] L. Zhang, M. Kan, S. Shan, and X. Chen. Prelar: World model pre-training with learnable action\\nrepresentation. In European Conference on Computer Vision, 2024.\\n[279] W. Zhang, P. Torr, M. Elhoseiny, and A. Bibi. Bi-factorial preference optimization: Balancing\\nsafety-helpfulness in language models. In International Conference on Learning Representations,\\n2025.\\n[280] X. Zhang and A. Boularias. One-shot imitation learning with invariance matching for robotic\\nmanipulation. In Robotics: Science and Systems, 2024.\\n[281] Y. Zhang, Z. Wang, and J. Shang. Clusterllm: Large language models as a guide for text clustering.\\narXiv preprint arXiv:2305.14871, 2023.\\n[282] Y. Zhang, J. Jia, X. Chen, A. Chen, Y. Zhang, J. Liu, K. Ding, and S. Liu. To generate or not?\\nsafety-driven unlearned diffusion models are still easy to generate unsafe images ... for now. In\\nEuropean Conference on Computer Vision, 2024.\\n[283] Y. Zhang*, E. Tzeng, Y. Du, and D. Kislyuk*. Large-scale reinforcement learning for diffusion\\nmodels. In European Conference on Computer Vision, 2024.\\n51'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 51}, page_content='[284] Z. Zhang, H. Liu, J. Chen, and X. Xu. Gooddrag: Towards good practices for drag editing with\\ndiffusion models. In International Conference on Learning Representations, 2025.\\n[285] B. Zhao, S. Yu, W. Ma, M. Yu, S. Mei, A. Wang, J. He, A. Yuille, and A. Kortylewski. Ood-cv: A\\nbenchmark for robustness to out-of-distribution shifts of individual nuisances in natural images. In\\nEuropean Conference on Computer Vision, 2024.\\n[286] T. Zhao, Y. Chen, Y. Wu, T. Liu, B. Du, P. Xiao, S. Qiu, H. Yang, G. Li, Y. Yang, and Y. Lin.\\nImproving bird’s eye view semantic segmentation by task decomposition. In Conference on\\nComputer Vision and Pattern Recognition, 2024.\\n[287] W. Zhao, P. Ding, Z. Min, Z. Gong, S. Bai, H. Zhao, and D. Wang. Vlas: Vision-language-action\\nmodel with speech instructions for customized robot manipulation. In International Conference on\\nLearning Representations, 2025.\\n[288] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, et al.\\nA survey of large language models. arXiv preprint arXiv:2303.18223, 2023.\\n[289] Y. Zhao, S. Dang, H. Ye, G. Dai, Y. Qian, and I. Tsang. Second-order fine-tuning without pain\\nfor llms: A hessian informed zeroth-order optimizer. In International Conference on Learning\\nRepresentations, 2025.\\n[290] Y. Zhao, M. Uehara, G. Scalia, S. Kung, T. Biancalani, S. Levine, and E. Hajiramezanali. Adding\\nconditional control to diffusion models with reinforcement learning. In International Conference\\non Learning Representations, 2025.\\n[291] Y. Zhao, W. Zhang, Y. Xie, A. Goyal, K. Kawaguchi, and M. Shieh. Understanding and enhancing\\nsafety mechanisms of llms via safety-specific neuron. In International Conference on Learning\\nRepresentations, 2025.\\n[292] J. Zheng, H. Wang, A. Zhang, T. D. Nguyen, J. Sun, and T.-S. Chua. Ali-agent: Assessing\\nllms’ alignment with human values via agent-based evaluation. In Neural Information Processing\\nSystems, 2024.\\n[293] R. Zheng, Y. Liang, S. Huang, J. Gao, H. D. III, A. Kolobov, F. Huang, and J. Yang. Tracevla:\\nVisual trace prompting enhances spatial-temporal awareness for generalist robotic policies. In\\nInternational Conference on Learning Representations, 2025.\\n[294] W. Zheng, W. Chen, Y. Huang, B. Zhang, Y. Duan, and J. Lu. Occworld: Learning a 3d occupancy\\nworld model for autonomous driving. In European Conference on Computer Vision, 2024.\\n[295] X. Zheng*, Y. Lyu, jiazhou zhou, and L. Wang*. Centering the value of every modality: Towards\\nefficient and resilient modality-agnostic semantic segmentation. In European Conference on\\nComputer Vision, 2024.\\n[296] X. Zheng*, Y. Lyu, and L. Wang*. Learning modality-agnostic representation for semantic\\nsegmentation from any modalities. In European Conference on Computer Vision, 2024.\\n[297] Z. Zhong, J. Cao, songen gu, S. Xie, L. Luo, H. Zhao, G. Zhou, H. Li, and Z. Yan*. Structured-nerf:\\nHierarchical scene graph with neural representation. In European Conference on Computer Vision,\\n2024.\\n[298] C. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan, L. He, et al. A comprehensive\\nsurvey on pretrained foundation models: A history from bert to chatgpt. International Journal of\\nMachine Learning and Cybernetics, pages 1–65, 2024.\\n52'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../../data/pdf/2510.20809v1.pdf', 'file_path': '../../data/pdf/2510.20809v1.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': 'Real Deep Research for AI, Robotics and Beyond', 'author': 'Xueyan Zou; Jianglong Ye; Hao Zhang; Xiaoyu Xiang; Mingyu Ding; Zhaojing Yang; Yong Jae Lee; Zhuowen Tu; Sifei Liu; Xiaolong Wang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 52}, page_content='[299] C. Zhou, X. Liu, F. Luo, and S. Huang. Latent radiance fields with 3d-aware 2d representations. In\\nInternational Conference on Learning Representations, 2025.\\n[300] C. Zhou, M. Zhang, P. Chen, C. Fu, Y. Shen, X. Zheng, X. Sun, and R. Ji. Learning interleaved\\nimage-text comprehension in vision-language large models. In International Conference on\\nLearning Representations, 2025.\\n[301] Z. Zhou, P. Atreya, A. Lee, H. R. Walke, O. Mees, and S. Levine. Autonomous improvement of\\ninstruction following skills via foundation models. In Conference on Robot Learning, 2024.\\n[302] Z. Zhuang, S. Yao, and H. Zhao. Humanoid parkour learning. In Conference on Robot Learning,\\n2024.\\n[303] W. Zimmer, G. A. Wardana, S. Sritharan, X. Zhou, R. Song, and A. C. Knoll. Tumtraf v2x\\ncooperative perception dataset. In Conference on Computer Vision and Pattern Recognition, 2024.\\n[304] A. Zou, L. Phan, J. Wang, D. Duenas, M. Lin, M. Andriushchenko, J. Z. Kolter, M. Fredrikson, and\\nD. Hendrycks. Improving alignment and robustness with circuit breakers. In Neural Information\\nProcessing Systems, 2024.\\n[305] X. Zou, L. Li, J. Wang, J. Yang, M. Ding, J. Wei, Z. Yang, F. Li, H. Zhang, S. Liu, A. Aravinthan,\\nY. J. Lee, and L. Wang. Interfacing foundation models’ embeddings. In Neural Information\\nProcessing Systems, 2024.\\n[306] A. Zouitine, D. Bertoin, P. Clavier, M. Geist, and E. Rachelson. Time-constrained robust mdps. In\\nNeural Information Processing Systems, 2024.\\n53')]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pypdf_loader = PyMuPDFLoader(\"../../data/pdf/2510.20809v1.pdf\")\n",
    "    pypdf_docs = pypdf_loader.load()\n",
    "    print(pypdf_docs)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e2ba82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3855fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFProcessor:\n",
    "    \"\"\"Advanced PDF processing with error handling\"\"\"\n",
    "    def __init__(self, chunk_size=1000, chunk_overlap=100):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size, \n",
    "            chunk_overlap = self.chunk_overlap, \n",
    "            separators=[\" \"]\n",
    "        )\n",
    "    def process_pdf(self, pdf_path):\n",
    "        # Loader\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        pages = loader.load()\n",
    "\n",
    "        # Splitter Chunking\n",
    "        chunks = []\n",
    "\n",
    "        for page_num, page in enumerate(pages):\n",
    "            page_content = self._clean_text(page.page_content)\n",
    "            page_chunk = self.splitter.create_documents(\n",
    "                texts = [page_content],\n",
    "                metadatas= [{\n",
    "                    **page.metadata,\n",
    "                    \"page\": page_num + 1,\n",
    "                    \"total_pages\": len(pages) \n",
    "                }]\n",
    "            )\n",
    "\n",
    "            chunks.extend(page_chunk)\n",
    "        return chunks\n",
    "            \n",
    "    def _clean_text(self, text: str): \n",
    "        clean_text = \" \".join(text.split())\n",
    "        return clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34bc3b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = PDFProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab9febaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PDFProcessor at 0x280a77978c0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03732a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len = 207\n"
     ]
    }
   ],
   "source": [
    "docs_chunked = processor.process_pdf(\"../../data/pdf/2510.20809v1.pdf\")\n",
    "print(f\"len = {len(docs_chunked)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9727179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Real Deep Research for AI, Robotics and Beyond Xueyan Zou∗1, Jianglong Ye∗1 , Hao Zhang2, Xiaoyu Xiang3, Mingyu Ding5, Zhaojing Yang1, Yong Jae Lee4, Zhuowen Tu1, Sifei Liu2, Xiaolong Wang1 1UC San Diego , 2NVIDIA , 3META , 4UW-Madison , 5UNC , https://realdeepresearch.github.io/ Abstract With the rapid growth of research in modern AI and robotics—now producing over 10,000 papers annually—it has become increasingly difficult for researchers to stay up to date. Fast-evolving trends,the rise of interdisciplinary work, andthe need to explore domains beyond one’s expertiseall contribute to this challenge. To address these issues, we propose a generalizable pipeline capable of systematically analyzing any research area: identifying emerging trends, uncovering cross-domain opportunities, and offering concrete starting points for new inquiry. In this work, we presentReal Deep Research(RDR)—a comprehensive framework applied to the domains of AI and robotics, with a particular focus on'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_chunked[0].page_content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
